{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T14:41:05.183508Z",
     "start_time": "2025-09-20T14:41:01.021220Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import parallel_backend\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "DATA_DIR = Path(\"../../../data/processed\")\n",
    "train = pd.read_csv(DATA_DIR / 'train_fe_scaled.csv')\n",
    "val = pd.read_csv(DATA_DIR / 'val_fe_scaled.csv')\n",
    "\n",
    "TARGET = 'Calories'\n",
    "FEATURES = [c for c in train.columns if c not in ['id', TARGET]]\n",
    "X_train, y_train = train[FEATURES], train[TARGET]\n",
    "X_val, y_val = val[FEATURES], val[TARGET]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea6b4134e36fe27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T14:41:05.213981Z",
     "start_time": "2025-09-20T14:41:05.201055Z"
    }
   },
   "outputs": [],
   "source": [
    "def report_results(model_name, grid, X_val, y_val):\n",
    "    best = grid.best_estimator_\n",
    "    preds = best.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    mse = mean_squared_error(y_val, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_val, preds)\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"best_params\": grid.best_params_,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2\n",
    "    }, best\n",
    "\n",
    "RESULTS_CSV = \"../../results/baseline_results.csv\"\n",
    "\n",
    "def append_result_to_csv(result_dict, csv_path=RESULTS_CSV):\n",
    "    df_new = pd.DataFrame([result_dict])\n",
    "    if os.path.exists(csv_path):\n",
    "        df_existing = pd.read_csv(csv_path)\n",
    "        df_all = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "    else:\n",
    "        df_all = df_new\n",
    "    df_all.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7fd4f688488c75c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:33:13.762798Z",
     "start_time": "2025-09-20T14:41:05.505239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Iteration 1, loss = 223.62951231\n",
      "Iteration 2, loss = 6.96271223\n",
      "Iteration 3, loss = 6.79865477\n",
      "Iteration 4, loss = 6.73301307\n",
      "Iteration 5, loss = 6.70652268\n",
      "Iteration 6, loss = 6.67003773\n",
      "Iteration 7, loss = 6.63140040\n",
      "Iteration 8, loss = 6.60778383\n",
      "Iteration 9, loss = 6.61824473\n",
      "Iteration 10, loss = 6.57107551\n",
      "Iteration 11, loss = 6.59264477\n",
      "Iteration 12, loss = 6.57879926\n",
      "Iteration 13, loss = 6.56376429\n",
      "Iteration 14, loss = 6.54580840\n",
      "Iteration 15, loss = 6.53925177\n",
      "Iteration 16, loss = 6.52270840\n",
      "Iteration 17, loss = 6.51567777\n",
      "Iteration 18, loss = 6.50623987\n",
      "Iteration 19, loss = 6.47957608\n",
      "Iteration 20, loss = 6.48289757\n",
      "Iteration 21, loss = 6.48980218\n",
      "Iteration 22, loss = 6.46797735\n",
      "Iteration 23, loss = 6.47871133\n",
      "Iteration 24, loss = 6.46372478\n",
      "Iteration 25, loss = 6.45879164\n",
      "Iteration 26, loss = 6.44505151\n",
      "Iteration 27, loss = 6.44333137\n",
      "Iteration 28, loss = 6.41420816\n",
      "Iteration 29, loss = 6.42033132\n",
      "Iteration 30, loss = 6.41389442\n",
      "Iteration 31, loss = 6.40390110\n",
      "Iteration 32, loss = 6.40989833\n",
      "Iteration 33, loss = 6.41566092\n",
      "Iteration 34, loss = 6.39189496\n",
      "Iteration 35, loss = 6.38806165\n",
      "Iteration 36, loss = 6.38483907\n",
      "Iteration 37, loss = 6.37219164\n",
      "Iteration 38, loss = 6.36104142\n",
      "Iteration 39, loss = 6.37062224\n",
      "Iteration 40, loss = 6.37339494\n",
      "Iteration 41, loss = 6.35631746\n",
      "Iteration 42, loss = 6.36412706\n",
      "Iteration 43, loss = 6.35964548\n",
      "Iteration 44, loss = 6.33668305\n",
      "Iteration 45, loss = 6.34605808\n",
      "Iteration 46, loss = 6.34070612\n",
      "Iteration 47, loss = 6.33217247\n",
      "Iteration 48, loss = 6.34489181\n",
      "Iteration 49, loss = 6.32956694\n",
      "Iteration 50, loss = 6.33543670\n",
      "Iteration 51, loss = 6.32541169\n",
      "Iteration 52, loss = 6.29735393\n",
      "Iteration 53, loss = 6.32956877\n",
      "Iteration 54, loss = 6.31690964\n",
      "Iteration 55, loss = 6.29535964\n",
      "Iteration 56, loss = 6.29657677\n",
      "Iteration 57, loss = 6.30929698\n",
      "Iteration 58, loss = 6.28036810\n",
      "Iteration 59, loss = 6.29750777\n",
      "Iteration 60, loss = 6.29238436\n",
      "Iteration 61, loss = 6.28716805\n",
      "Iteration 62, loss = 6.27913744\n",
      "Iteration 63, loss = 6.28023249\n",
      "Iteration 64, loss = 6.27335320\n",
      "Iteration 65, loss = 6.27272131\n",
      "Iteration 66, loss = 6.26804761\n",
      "Iteration 67, loss = 6.27458365\n",
      "Iteration 68, loss = 6.26240311\n",
      "Iteration 69, loss = 6.25583399\n",
      "Iteration 70, loss = 6.27878887\n",
      "Iteration 71, loss = 6.25777130\n",
      "Iteration 72, loss = 6.24777985\n",
      "Iteration 73, loss = 6.25732315\n",
      "Iteration 74, loss = 6.25441258\n",
      "Iteration 75, loss = 6.23144926\n",
      "Iteration 76, loss = 6.24377573\n",
      "Iteration 77, loss = 6.25112211\n",
      "Iteration 78, loss = 6.24151513\n",
      "Iteration 79, loss = 6.25444948\n",
      "Iteration 80, loss = 6.24300306\n",
      "Iteration 81, loss = 6.22791002\n",
      "Iteration 82, loss = 6.22571101\n",
      "Iteration 83, loss = 6.24121969\n",
      "Iteration 84, loss = 6.23409608\n",
      "Iteration 85, loss = 6.24039923\n",
      "Iteration 86, loss = 6.22610763\n",
      "Iteration 87, loss = 6.21759192\n",
      "Iteration 88, loss = 6.21524565\n",
      "Iteration 89, loss = 6.22753060\n",
      "Iteration 90, loss = 6.21907128\n",
      "Iteration 91, loss = 6.22279721\n",
      "Iteration 92, loss = 6.21623396\n",
      "Iteration 93, loss = 6.20742241\n",
      "Iteration 94, loss = 6.20568446\n",
      "Iteration 95, loss = 6.21010199\n",
      "Iteration 96, loss = 6.19176908\n",
      "Iteration 97, loss = 6.19981272\n",
      "Iteration 98, loss = 6.19843517\n",
      "Iteration 99, loss = 6.20447381\n",
      "Iteration 100, loss = 6.18739136\n",
      "Iteration 101, loss = 6.20029154\n",
      "Iteration 102, loss = 6.19193123\n",
      "Iteration 103, loss = 6.20258678\n",
      "Iteration 104, loss = 6.18736466\n",
      "Iteration 105, loss = 6.19293635\n",
      "Iteration 106, loss = 6.18060464\n",
      "Iteration 107, loss = 6.17263090\n",
      "Iteration 108, loss = 6.19287754\n",
      "Iteration 109, loss = 6.18257282\n",
      "Iteration 110, loss = 6.18414756\n",
      "Iteration 111, loss = 6.18200227\n",
      "Iteration 112, loss = 6.18193375\n",
      "Iteration 113, loss = 6.17955300\n",
      "Iteration 114, loss = 6.17409819\n",
      "Iteration 115, loss = 6.18029138\n",
      "Iteration 116, loss = 6.16367857\n",
      "Iteration 117, loss = 6.17731706\n",
      "Iteration 118, loss = 6.17071456\n",
      "Iteration 119, loss = 6.18202970\n",
      "Iteration 120, loss = 6.15962130\n",
      "Iteration 121, loss = 6.15092514\n",
      "Iteration 122, loss = 6.18249728\n",
      "Iteration 123, loss = 6.15766371\n",
      "Iteration 124, loss = 6.15587033\n",
      "Iteration 125, loss = 6.16441760\n",
      "Iteration 126, loss = 6.15810970\n",
      "Iteration 127, loss = 6.13436973\n",
      "Iteration 128, loss = 6.15090821\n",
      "Iteration 129, loss = 6.16570059\n",
      "Iteration 130, loss = 6.14623786\n",
      "Iteration 131, loss = 6.14631844\n",
      "Iteration 132, loss = 6.13381845\n",
      "Iteration 133, loss = 6.15713038\n",
      "Iteration 134, loss = 6.13375178\n",
      "Iteration 135, loss = 6.13994628\n",
      "Iteration 136, loss = 6.13697038\n",
      "Iteration 137, loss = 6.12989632\n",
      "Iteration 138, loss = 6.13485457\n",
      "Iteration 139, loss = 6.14216938\n",
      "Iteration 140, loss = 6.12621352\n",
      "Iteration 141, loss = 6.12523155\n",
      "Iteration 142, loss = 6.14190070\n",
      "Iteration 143, loss = 6.11744603\n",
      "Iteration 144, loss = 6.13102924\n",
      "Iteration 145, loss = 6.12785168\n",
      "Iteration 146, loss = 6.12237486\n",
      "Iteration 147, loss = 6.13470272\n",
      "Iteration 148, loss = 6.11232752\n",
      "Iteration 149, loss = 6.12017524\n",
      "Iteration 150, loss = 6.11178067\n",
      "Iteration 151, loss = 6.10572761\n",
      "Iteration 152, loss = 6.11573065\n",
      "Iteration 153, loss = 6.10896244\n",
      "Iteration 154, loss = 6.11160165\n",
      "Iteration 155, loss = 6.11980841\n",
      "Iteration 156, loss = 6.10955867\n",
      "Iteration 157, loss = 6.10698383\n",
      "Iteration 158, loss = 6.12559861\n",
      "Iteration 159, loss = 6.09889374\n",
      "Iteration 160, loss = 6.08978489\n",
      "Iteration 161, loss = 6.09242435\n",
      "Iteration 162, loss = 6.10507228\n",
      "Iteration 163, loss = 6.09921317\n",
      "Iteration 164, loss = 6.08322390\n",
      "Iteration 165, loss = 6.07204867\n",
      "Iteration 166, loss = 6.08077135\n",
      "Iteration 167, loss = 6.09804486\n",
      "Iteration 168, loss = 6.07534601\n",
      "Iteration 169, loss = 6.07928983\n",
      "Iteration 170, loss = 6.07642742\n",
      "Iteration 171, loss = 6.07311904\n",
      "Iteration 172, loss = 6.05936189\n",
      "Iteration 173, loss = 6.07512735\n",
      "Iteration 174, loss = 6.07074403\n",
      "Iteration 175, loss = 6.06735768\n",
      "Iteration 176, loss = 6.06723551\n",
      "Iteration 177, loss = 6.06689471\n",
      "Iteration 178, loss = 6.06922577\n",
      "Iteration 179, loss = 6.04472643\n",
      "Iteration 180, loss = 6.06370279\n",
      "Iteration 181, loss = 6.04350087\n",
      "Iteration 182, loss = 6.03207945\n",
      "Iteration 183, loss = 6.05654341\n",
      "Iteration 184, loss = 6.04083476\n",
      "Iteration 185, loss = 6.03619791\n",
      "Iteration 186, loss = 6.04600817\n",
      "Iteration 187, loss = 6.02511904\n",
      "Iteration 188, loss = 6.04135666\n",
      "Iteration 189, loss = 6.04009886\n",
      "Iteration 190, loss = 6.04003491\n",
      "Iteration 191, loss = 6.03540146\n",
      "Iteration 192, loss = 6.03030018\n",
      "Iteration 193, loss = 6.01899124\n",
      "Iteration 194, loss = 6.03255213\n",
      "Iteration 195, loss = 6.01763434\n",
      "Iteration 196, loss = 6.01844308\n",
      "Iteration 197, loss = 6.03458166\n",
      "Iteration 198, loss = 5.99904753\n",
      "Iteration 199, loss = 6.02210184\n",
      "Iteration 200, loss = 5.99995020\n",
      "Iteration 201, loss = 6.01685895\n",
      "Iteration 202, loss = 6.01383739\n",
      "Iteration 203, loss = 5.99411806\n",
      "Iteration 204, loss = 5.98775001\n",
      "Iteration 205, loss = 6.00394706\n",
      "Iteration 206, loss = 5.99134764\n",
      "Iteration 207, loss = 5.99922162\n",
      "Iteration 208, loss = 5.99247030\n",
      "Iteration 209, loss = 5.98030853\n",
      "Iteration 210, loss = 6.00210185\n",
      "Iteration 211, loss = 5.96997985\n",
      "Iteration 212, loss = 5.97699540\n",
      "Iteration 213, loss = 5.98725149\n",
      "Iteration 214, loss = 5.97086533\n",
      "Iteration 215, loss = 5.98688946\n",
      "Iteration 216, loss = 5.97743610\n",
      "Iteration 217, loss = 5.96617798\n",
      "Iteration 218, loss = 5.96628466\n",
      "Iteration 219, loss = 5.97576892\n",
      "Iteration 220, loss = 5.95651621\n",
      "Iteration 221, loss = 5.96837612\n",
      "Iteration 222, loss = 5.94671712\n",
      "Iteration 223, loss = 5.95752318\n",
      "Iteration 224, loss = 5.96541679\n",
      "Iteration 225, loss = 5.96287449\n",
      "Iteration 226, loss = 5.95402469\n",
      "Iteration 227, loss = 5.94940902\n",
      "Iteration 228, loss = 5.95856098\n",
      "Iteration 229, loss = 5.95103702\n",
      "Iteration 230, loss = 5.96391520\n",
      "Iteration 231, loss = 5.94603029\n",
      "Iteration 232, loss = 5.95196234\n",
      "Iteration 233, loss = 5.93818849\n",
      "Iteration 234, loss = 5.94067652\n",
      "Iteration 235, loss = 5.94404184\n",
      "Iteration 236, loss = 5.93138080\n",
      "Iteration 237, loss = 5.93986966\n",
      "Iteration 238, loss = 5.93445279\n",
      "Iteration 239, loss = 5.92999831\n",
      "Iteration 240, loss = 5.92960653\n",
      "Iteration 241, loss = 5.93718962\n",
      "Iteration 242, loss = 5.93728134\n",
      "Iteration 243, loss = 5.93042835\n",
      "Iteration 244, loss = 5.92300118\n",
      "Iteration 245, loss = 5.94207060\n",
      "Iteration 246, loss = 5.93622916\n",
      "Iteration 247, loss = 5.92440887\n",
      "Iteration 248, loss = 5.93289521\n",
      "Iteration 249, loss = 5.92421123\n",
      "Iteration 250, loss = 5.91307211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Downloads\\IMF\\Modulos\\Aun por resolver\\TFM\\TFM\\calories\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, max_iter=250;, score=-13.658 total time=16.8min\n",
      "Iteration 1, loss = 225.85352978\n",
      "Iteration 2, loss = 7.28850271\n",
      "Iteration 3, loss = 7.11331602\n",
      "Iteration 4, loss = 7.03993440\n",
      "Iteration 5, loss = 7.00271579\n",
      "Iteration 6, loss = 6.94260117\n",
      "Iteration 7, loss = 6.90595007\n",
      "Iteration 8, loss = 6.89613770\n",
      "Iteration 9, loss = 6.84966551\n",
      "Iteration 10, loss = 6.83129233\n",
      "Iteration 11, loss = 6.82670499\n",
      "Iteration 12, loss = 6.80604693\n",
      "Iteration 13, loss = 6.83136105\n",
      "Iteration 14, loss = 6.76155953\n",
      "Iteration 15, loss = 6.77464202\n",
      "Iteration 16, loss = 6.74075055\n",
      "Iteration 17, loss = 6.73702749\n",
      "Iteration 18, loss = 6.73447076\n",
      "Iteration 19, loss = 6.70447250\n",
      "Iteration 20, loss = 6.69720045\n",
      "Iteration 21, loss = 6.71690923\n",
      "Iteration 22, loss = 6.69856135\n",
      "Iteration 23, loss = 6.68395698\n",
      "Iteration 24, loss = 6.67128297\n",
      "Iteration 25, loss = 6.65465315\n",
      "Iteration 26, loss = 6.63705279\n",
      "Iteration 27, loss = 6.63115086\n",
      "Iteration 28, loss = 6.62828585\n",
      "Iteration 29, loss = 6.61724388\n",
      "Iteration 30, loss = 6.60357118\n",
      "Iteration 31, loss = 6.58827680\n",
      "Iteration 32, loss = 6.60549916\n",
      "Iteration 33, loss = 6.58826846\n",
      "Iteration 34, loss = 6.57045703\n",
      "Iteration 35, loss = 6.59006981\n",
      "Iteration 36, loss = 6.55228170\n",
      "Iteration 37, loss = 6.55242452\n",
      "Iteration 38, loss = 6.55323955\n",
      "Iteration 39, loss = 6.54679367\n",
      "Iteration 40, loss = 6.54138264\n",
      "Iteration 41, loss = 6.55005388\n",
      "Iteration 42, loss = 6.52667926\n",
      "Iteration 43, loss = 6.52770211\n",
      "Iteration 44, loss = 6.53711259\n",
      "Iteration 45, loss = 6.51844837\n",
      "Iteration 46, loss = 6.50202112\n",
      "Iteration 47, loss = 6.50921398\n",
      "Iteration 48, loss = 6.51199198\n",
      "Iteration 49, loss = 6.49936492\n",
      "Iteration 50, loss = 6.50230099\n",
      "Iteration 51, loss = 6.48344181\n",
      "Iteration 52, loss = 6.49327939\n",
      "Iteration 53, loss = 6.47572243\n",
      "Iteration 54, loss = 6.48218957\n",
      "Iteration 55, loss = 6.47305958\n",
      "Iteration 56, loss = 6.45796006\n",
      "Iteration 57, loss = 6.47810191\n",
      "Iteration 58, loss = 6.46009164\n",
      "Iteration 59, loss = 6.47323571\n",
      "Iteration 60, loss = 6.44444262\n",
      "Iteration 61, loss = 6.44818458\n",
      "Iteration 62, loss = 6.44225078\n",
      "Iteration 63, loss = 6.43904940\n",
      "Iteration 64, loss = 6.44306488\n",
      "Iteration 65, loss = 6.43605459\n",
      "Iteration 66, loss = 6.42873892\n",
      "Iteration 67, loss = 6.43869258\n",
      "Iteration 68, loss = 6.44108083\n",
      "Iteration 69, loss = 6.42872793\n",
      "Iteration 70, loss = 6.43052465\n",
      "Iteration 71, loss = 6.43327932\n",
      "Iteration 72, loss = 6.41838356\n",
      "Iteration 73, loss = 6.41394543\n",
      "Iteration 74, loss = 6.41821414\n",
      "Iteration 75, loss = 6.40274780\n",
      "Iteration 76, loss = 6.41715748\n",
      "Iteration 77, loss = 6.39108207\n",
      "Iteration 78, loss = 6.40456969\n",
      "Iteration 79, loss = 6.39720482\n",
      "Iteration 80, loss = 6.41842713\n",
      "Iteration 81, loss = 6.39265473\n",
      "Iteration 82, loss = 6.39590303\n",
      "Iteration 83, loss = 6.40608833\n",
      "Iteration 84, loss = 6.39462241\n",
      "Iteration 85, loss = 6.39635042\n",
      "Iteration 86, loss = 6.39363785\n",
      "Iteration 87, loss = 6.38947076\n",
      "Iteration 88, loss = 6.38202409\n",
      "Iteration 89, loss = 6.40168018\n",
      "Iteration 90, loss = 6.37151010\n",
      "Iteration 91, loss = 6.37371230\n",
      "Iteration 92, loss = 6.38408593\n",
      "Iteration 93, loss = 6.38411081\n",
      "Iteration 94, loss = 6.37822418\n",
      "Iteration 95, loss = 6.37892575\n",
      "Iteration 96, loss = 6.36498542\n",
      "Iteration 97, loss = 6.36769520\n",
      "Iteration 98, loss = 6.37234839\n",
      "Iteration 99, loss = 6.36399629\n",
      "Iteration 100, loss = 6.35255054\n",
      "Iteration 101, loss = 6.35597042\n",
      "Iteration 102, loss = 6.35732110\n",
      "Iteration 103, loss = 6.37836003\n",
      "Iteration 104, loss = 6.34491474\n",
      "Iteration 105, loss = 6.34589701\n",
      "Iteration 106, loss = 6.34962407\n",
      "Iteration 107, loss = 6.34413293\n",
      "Iteration 108, loss = 6.34586456\n",
      "Iteration 109, loss = 6.33740315\n",
      "Iteration 110, loss = 6.34248469\n",
      "Iteration 111, loss = 6.31526389\n",
      "Iteration 112, loss = 6.34731403\n",
      "Iteration 113, loss = 6.35062686\n",
      "Iteration 114, loss = 6.33478264\n",
      "Iteration 115, loss = 6.33956536\n",
      "Iteration 116, loss = 6.32444622\n",
      "Iteration 117, loss = 6.33308524\n",
      "Iteration 118, loss = 6.33539935\n",
      "Iteration 119, loss = 6.32928316\n",
      "Iteration 120, loss = 6.30551537\n",
      "Iteration 121, loss = 6.32412842\n",
      "Iteration 122, loss = 6.32398213\n",
      "Iteration 123, loss = 6.30809410\n",
      "Iteration 124, loss = 6.30747815\n",
      "Iteration 125, loss = 6.32330766\n",
      "Iteration 126, loss = 6.30578443\n",
      "Iteration 127, loss = 6.30717355\n",
      "Iteration 128, loss = 6.30302742\n",
      "Iteration 129, loss = 6.30828583\n",
      "Iteration 130, loss = 6.31201506\n",
      "Iteration 131, loss = 6.29971625\n",
      "Iteration 132, loss = 6.28657922\n",
      "Iteration 133, loss = 6.31629205\n",
      "Iteration 134, loss = 6.29209373\n",
      "Iteration 135, loss = 6.31372560\n",
      "Iteration 136, loss = 6.29474005\n",
      "Iteration 137, loss = 6.28925618\n",
      "Iteration 138, loss = 6.28794828\n",
      "Iteration 139, loss = 6.30146790\n",
      "Iteration 140, loss = 6.29012987\n",
      "Iteration 141, loss = 6.27109525\n",
      "Iteration 142, loss = 6.29101229\n",
      "Iteration 143, loss = 6.28440432\n",
      "Iteration 144, loss = 6.28476399\n",
      "Iteration 145, loss = 6.26609499\n",
      "Iteration 146, loss = 6.28628405\n",
      "Iteration 147, loss = 6.29649496\n",
      "Iteration 148, loss = 6.26949922\n",
      "Iteration 149, loss = 6.28252401\n",
      "Iteration 150, loss = 6.26915283\n",
      "Iteration 151, loss = 6.25819095\n",
      "Iteration 152, loss = 6.27735008\n",
      "Iteration 153, loss = 6.26808390\n",
      "Iteration 154, loss = 6.25430482\n",
      "Iteration 155, loss = 6.26252630\n",
      "Iteration 156, loss = 6.27209156\n",
      "Iteration 157, loss = 6.29016141\n",
      "Iteration 158, loss = 6.25082957\n",
      "Iteration 159, loss = 6.27801489\n",
      "Iteration 160, loss = 6.25809412\n",
      "Iteration 161, loss = 6.25261397\n",
      "Iteration 162, loss = 6.26561241\n",
      "Iteration 163, loss = 6.24163832\n",
      "Iteration 164, loss = 6.24166939\n",
      "Iteration 165, loss = 6.24278033\n",
      "Iteration 166, loss = 6.25492041\n",
      "Iteration 167, loss = 6.25756441\n",
      "Iteration 168, loss = 6.25334019\n",
      "Iteration 169, loss = 6.25440564\n",
      "Iteration 170, loss = 6.26453249\n",
      "Iteration 171, loss = 6.23840888\n",
      "Iteration 172, loss = 6.24819562\n",
      "Iteration 173, loss = 6.25801417\n",
      "Iteration 174, loss = 6.24386818\n",
      "Iteration 175, loss = 6.25066942\n",
      "Iteration 176, loss = 6.25381866\n",
      "Iteration 177, loss = 6.24042314\n",
      "Iteration 178, loss = 6.23184904\n",
      "Iteration 179, loss = 6.22426928\n",
      "Iteration 180, loss = 6.23795716\n",
      "Iteration 181, loss = 6.22436592\n",
      "Iteration 182, loss = 6.23568865\n",
      "Iteration 183, loss = 6.22869787\n",
      "Iteration 184, loss = 6.21901283\n",
      "Iteration 185, loss = 6.21969354\n",
      "Iteration 186, loss = 6.23372766\n",
      "Iteration 187, loss = 6.21648064\n",
      "Iteration 188, loss = 6.22089574\n",
      "Iteration 189, loss = 6.22082936\n",
      "Iteration 190, loss = 6.22499421\n",
      "Iteration 191, loss = 6.21820350\n",
      "Iteration 192, loss = 6.21308668\n",
      "Iteration 193, loss = 6.21011401\n",
      "Iteration 194, loss = 6.22019908\n",
      "Iteration 195, loss = 6.21004780\n",
      "Iteration 196, loss = 6.21790008\n",
      "Iteration 197, loss = 6.21719968\n",
      "Iteration 198, loss = 6.20550922\n",
      "Iteration 199, loss = 6.20548664\n",
      "Iteration 200, loss = 6.21917114\n",
      "Iteration 201, loss = 6.20504836\n",
      "Iteration 202, loss = 6.20951068\n",
      "Iteration 203, loss = 6.19129043\n",
      "Iteration 204, loss = 6.19878861\n",
      "Iteration 205, loss = 6.20559581\n",
      "Iteration 206, loss = 6.19999501\n",
      "Iteration 207, loss = 6.20372843\n",
      "Iteration 208, loss = 6.19947164\n",
      "Iteration 209, loss = 6.19704613\n",
      "Iteration 210, loss = 6.20961936\n",
      "Iteration 211, loss = 6.18479321\n",
      "Iteration 212, loss = 6.19569032\n",
      "Iteration 213, loss = 6.18801230\n",
      "Iteration 214, loss = 6.19320241\n",
      "Iteration 215, loss = 6.18149411\n",
      "Iteration 216, loss = 6.18452673\n",
      "Iteration 217, loss = 6.18975927\n",
      "Iteration 218, loss = 6.18258436\n",
      "Iteration 219, loss = 6.18740178\n",
      "Iteration 220, loss = 6.19304623\n",
      "Iteration 221, loss = 6.17108612\n",
      "Iteration 222, loss = 6.18443233\n",
      "Iteration 223, loss = 6.17648411\n",
      "Iteration 224, loss = 6.15630791\n",
      "Iteration 225, loss = 6.17644994\n",
      "Iteration 226, loss = 6.17800741\n",
      "Iteration 227, loss = 6.17106601\n",
      "Iteration 228, loss = 6.17339836\n",
      "Iteration 229, loss = 6.16215518\n",
      "Iteration 230, loss = 6.17057784\n",
      "Iteration 231, loss = 6.16078930\n",
      "Iteration 232, loss = 6.17695908\n",
      "Iteration 233, loss = 6.16974415\n",
      "Iteration 234, loss = 6.16197756\n",
      "Iteration 235, loss = 6.15551331\n",
      "Iteration 236, loss = 6.15690183\n",
      "Iteration 237, loss = 6.15874817\n",
      "Iteration 238, loss = 6.15921810\n",
      "Iteration 239, loss = 6.15590412\n",
      "Iteration 240, loss = 6.16826251\n",
      "Iteration 241, loss = 6.16538465\n",
      "Iteration 242, loss = 6.15743200\n",
      "Iteration 243, loss = 6.14892039\n",
      "Iteration 244, loss = 6.15322401\n",
      "Iteration 245, loss = 6.14626229\n",
      "Iteration 246, loss = 6.14885276\n",
      "Iteration 247, loss = 6.15108715\n",
      "Iteration 248, loss = 6.15062550\n",
      "Iteration 249, loss = 6.14056045\n",
      "Iteration 250, loss = 6.14130294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Downloads\\IMF\\Modulos\\Aun por resolver\\TFM\\TFM\\calories\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, max_iter=250;, score=-12.761 total time=29.8min\n",
      "Iteration 1, loss = 222.99107231\n",
      "Iteration 2, loss = 7.13891874\n",
      "Iteration 3, loss = 6.97532210\n",
      "Iteration 4, loss = 6.90294276\n",
      "Iteration 5, loss = 6.90100095\n",
      "Iteration 6, loss = 6.84276370\n",
      "Iteration 7, loss = 6.80033162\n",
      "Iteration 8, loss = 6.81377697\n",
      "Iteration 9, loss = 6.76404790\n",
      "Iteration 10, loss = 6.76292437\n",
      "Iteration 11, loss = 6.75536819\n",
      "Iteration 12, loss = 6.74692538\n",
      "Iteration 13, loss = 6.72767345\n",
      "Iteration 14, loss = 6.71194373\n",
      "Iteration 15, loss = 6.70080057\n",
      "Iteration 16, loss = 6.68100108\n",
      "Iteration 17, loss = 6.65948667\n",
      "Iteration 18, loss = 6.65534896\n",
      "Iteration 19, loss = 6.64362887\n",
      "Iteration 20, loss = 6.64802183\n",
      "Iteration 21, loss = 6.62948591\n",
      "Iteration 22, loss = 6.61249382\n",
      "Iteration 23, loss = 6.62949627\n",
      "Iteration 24, loss = 6.61114873\n",
      "Iteration 25, loss = 6.60128470\n",
      "Iteration 26, loss = 6.60241044\n",
      "Iteration 27, loss = 6.57375079\n",
      "Iteration 28, loss = 6.59107424\n",
      "Iteration 29, loss = 6.59314605\n",
      "Iteration 30, loss = 6.55238439\n",
      "Iteration 31, loss = 6.58205744\n",
      "Iteration 32, loss = 6.56085702\n",
      "Iteration 33, loss = 6.56484575\n",
      "Iteration 34, loss = 6.53634441\n",
      "Iteration 35, loss = 6.54809113\n",
      "Iteration 36, loss = 6.53784852\n",
      "Iteration 37, loss = 6.53916609\n",
      "Iteration 38, loss = 6.51836378\n",
      "Iteration 39, loss = 6.52068952\n",
      "Iteration 40, loss = 6.51422874\n",
      "Iteration 41, loss = 6.51941209\n",
      "Iteration 42, loss = 6.50877740\n",
      "Iteration 43, loss = 6.50856020\n",
      "Iteration 44, loss = 6.49437891\n",
      "Iteration 45, loss = 6.49859748\n",
      "Iteration 46, loss = 6.51233237\n",
      "Iteration 47, loss = 6.48134477\n",
      "Iteration 48, loss = 6.47296411\n",
      "Iteration 49, loss = 6.47802385\n",
      "Iteration 50, loss = 6.46621868\n",
      "Iteration 51, loss = 6.45981638\n",
      "Iteration 52, loss = 6.45054539\n",
      "Iteration 53, loss = 6.44253667\n",
      "Iteration 54, loss = 6.45185781\n",
      "Iteration 55, loss = 6.43198118\n",
      "Iteration 56, loss = 6.42853995\n",
      "Iteration 57, loss = 6.43362093\n",
      "Iteration 58, loss = 6.42479820\n",
      "Iteration 59, loss = 6.41515493\n",
      "Iteration 60, loss = 6.39244473\n",
      "Iteration 61, loss = 6.39560657\n",
      "Iteration 62, loss = 6.38605537\n",
      "Iteration 63, loss = 6.40233521\n",
      "Iteration 64, loss = 6.38250826\n",
      "Iteration 65, loss = 6.37288268\n",
      "Iteration 66, loss = 6.37920725\n",
      "Iteration 67, loss = 6.36942078\n",
      "Iteration 68, loss = 6.38358705\n",
      "Iteration 69, loss = 6.35333354\n",
      "Iteration 70, loss = 6.37541188\n",
      "Iteration 71, loss = 6.35541065\n",
      "Iteration 72, loss = 6.36613157\n",
      "Iteration 73, loss = 6.35352289\n",
      "Iteration 74, loss = 6.34779889\n",
      "Iteration 75, loss = 6.33775111\n",
      "Iteration 76, loss = 6.33700557\n",
      "Iteration 77, loss = 6.32884903\n",
      "Iteration 78, loss = 6.33366612\n",
      "Iteration 79, loss = 6.32080630\n",
      "Iteration 80, loss = 6.29719626\n",
      "Iteration 81, loss = 6.31373411\n",
      "Iteration 82, loss = 6.33486251\n",
      "Iteration 83, loss = 6.32011991\n",
      "Iteration 84, loss = 6.31256005\n",
      "Iteration 85, loss = 6.31512482\n",
      "Iteration 86, loss = 6.29933106\n",
      "Iteration 87, loss = 6.30904890\n",
      "Iteration 88, loss = 6.30125860\n",
      "Iteration 89, loss = 6.30909388\n",
      "Iteration 90, loss = 6.28379798\n",
      "Iteration 91, loss = 6.30207112\n",
      "Iteration 92, loss = 6.28429264\n",
      "Iteration 93, loss = 6.29625347\n",
      "Iteration 94, loss = 6.30782498\n",
      "Iteration 95, loss = 6.25864782\n",
      "Iteration 96, loss = 6.28725898\n",
      "Iteration 97, loss = 6.28019391\n",
      "Iteration 98, loss = 6.28443977\n",
      "Iteration 99, loss = 6.27926979\n",
      "Iteration 100, loss = 6.26889966\n",
      "Iteration 101, loss = 6.28668653\n",
      "Iteration 102, loss = 6.25944968\n",
      "Iteration 103, loss = 6.27563417\n",
      "Iteration 104, loss = 6.26581329\n",
      "Iteration 105, loss = 6.26782493\n",
      "Iteration 106, loss = 6.24785737\n",
      "Iteration 107, loss = 6.26442687\n",
      "Iteration 108, loss = 6.26082749\n",
      "Iteration 109, loss = 6.24615375\n",
      "Iteration 110, loss = 6.26457663\n",
      "Iteration 111, loss = 6.24653505\n",
      "Iteration 112, loss = 6.26465319\n",
      "Iteration 113, loss = 6.24675477\n",
      "Iteration 114, loss = 6.25397542\n",
      "Iteration 115, loss = 6.25373916\n",
      "Iteration 116, loss = 6.22538452\n",
      "Iteration 117, loss = 6.24658753\n",
      "Iteration 118, loss = 6.23576613\n",
      "Iteration 119, loss = 6.23957414\n",
      "Iteration 120, loss = 6.25077611\n",
      "Iteration 121, loss = 6.22805068\n",
      "Iteration 122, loss = 6.23621998\n",
      "Iteration 123, loss = 6.23315880\n",
      "Iteration 124, loss = 6.24365172\n",
      "Iteration 125, loss = 6.24884174\n",
      "Iteration 126, loss = 6.22188326\n",
      "Iteration 127, loss = 6.23075696\n",
      "Iteration 128, loss = 6.21919261\n",
      "Iteration 129, loss = 6.22402391\n",
      "Iteration 130, loss = 6.22908916\n",
      "Iteration 131, loss = 6.21416780\n",
      "Iteration 132, loss = 6.21959599\n",
      "Iteration 133, loss = 6.21852140\n",
      "Iteration 134, loss = 6.21924574\n",
      "Iteration 135, loss = 6.22139997\n",
      "Iteration 136, loss = 6.21025427\n",
      "Iteration 137, loss = 6.18925435\n",
      "Iteration 138, loss = 6.19507654\n",
      "Iteration 139, loss = 6.20135995\n",
      "Iteration 140, loss = 6.20310735\n",
      "Iteration 141, loss = 6.19804701\n",
      "Iteration 142, loss = 6.20035133\n",
      "Iteration 143, loss = 6.19527435\n",
      "Iteration 144, loss = 6.20936941\n",
      "Iteration 145, loss = 6.19117251\n",
      "Iteration 146, loss = 6.20299024\n",
      "Iteration 147, loss = 6.21069007\n",
      "Iteration 148, loss = 6.18974486\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3] END activation=relu, hidden_layer_sizes=(100, 50), learning_rate_init=0.001, max_iter=250;, score=-12.963 total time=26.2min\n",
      "Iteration 1, loss = 1531.93309868\n",
      "Iteration 2, loss = 105.66962556\n",
      "Iteration 3, loss = 23.46675985\n",
      "Iteration 4, loss = 10.12919351\n",
      "Iteration 5, loss = 7.35663177\n",
      "Iteration 6, loss = 6.76349322\n",
      "Iteration 7, loss = 6.60391379\n",
      "Iteration 8, loss = 6.54471518\n",
      "Iteration 9, loss = 6.51033642\n",
      "Iteration 10, loss = 6.48948256\n",
      "Iteration 11, loss = 6.47038293\n",
      "Iteration 12, loss = 6.45343907\n",
      "Iteration 13, loss = 6.44439687\n",
      "Iteration 14, loss = 6.43186377\n",
      "Iteration 15, loss = 6.42006650\n",
      "Iteration 16, loss = 6.41839034\n",
      "Iteration 17, loss = 6.40701475\n",
      "Iteration 18, loss = 6.40043116\n",
      "Iteration 19, loss = 6.39105578\n",
      "Iteration 20, loss = 6.38814740\n",
      "Iteration 21, loss = 6.38161090\n",
      "Iteration 22, loss = 6.37271323\n",
      "Iteration 23, loss = 6.37268929\n",
      "Iteration 24, loss = 6.36662894\n",
      "Iteration 25, loss = 6.36366884\n",
      "Iteration 26, loss = 6.35463079\n",
      "Iteration 27, loss = 6.34938800\n",
      "Iteration 28, loss = 6.34819027\n",
      "Iteration 29, loss = 6.34210618\n",
      "Iteration 30, loss = 6.34335700\n",
      "Iteration 31, loss = 6.33279423\n",
      "Iteration 32, loss = 6.33784671\n",
      "Iteration 33, loss = 6.32026741\n",
      "Iteration 34, loss = 6.33541363\n",
      "Iteration 35, loss = 6.32165203\n",
      "Iteration 36, loss = 6.32110297\n",
      "Iteration 37, loss = 6.31558264\n",
      "Iteration 38, loss = 6.31545472\n",
      "Iteration 39, loss = 6.31029593\n",
      "Iteration 40, loss = 6.30601298\n",
      "Iteration 41, loss = 6.30812724\n",
      "Iteration 42, loss = 6.30128265\n",
      "Iteration 43, loss = 6.29849899\n",
      "Iteration 44, loss = 6.29321835\n",
      "Iteration 45, loss = 6.28996915\n",
      "Iteration 46, loss = 6.28799417\n",
      "Iteration 47, loss = 6.28628526\n",
      "Iteration 48, loss = 6.28154050\n",
      "Iteration 49, loss = 6.27954482\n",
      "Iteration 50, loss = 6.27691899\n",
      "Iteration 51, loss = 6.26919169\n",
      "Iteration 52, loss = 6.26782075\n",
      "Iteration 53, loss = 6.26692978\n",
      "Iteration 54, loss = 6.26618907\n",
      "Iteration 55, loss = 6.26285905\n",
      "Iteration 56, loss = 6.26394025\n",
      "Iteration 57, loss = 6.26087757\n",
      "Iteration 58, loss = 6.25669279\n",
      "Iteration 59, loss = 6.25688552\n",
      "Iteration 60, loss = 6.25282772\n",
      "Iteration 61, loss = 6.24964732\n",
      "Iteration 62, loss = 6.25223382\n",
      "Iteration 63, loss = 6.24624093\n",
      "Iteration 64, loss = 6.24222814\n",
      "Iteration 65, loss = 6.24207707\n",
      "Iteration 66, loss = 6.23878229\n",
      "Iteration 67, loss = 6.23766407\n",
      "Iteration 68, loss = 6.24141252\n",
      "Iteration 69, loss = 6.24044441\n",
      "Iteration 70, loss = 6.23734112\n",
      "Iteration 71, loss = 6.23611692\n",
      "Iteration 72, loss = 6.23596353\n",
      "Iteration 73, loss = 6.23193196\n",
      "Iteration 74, loss = 6.23043399\n",
      "Iteration 75, loss = 6.23650733\n",
      "Iteration 76, loss = 6.22920625\n",
      "Iteration 77, loss = 6.22955694\n",
      "Iteration 78, loss = 6.22975317\n",
      "Iteration 79, loss = 6.22625100\n",
      "Iteration 80, loss = 6.22371578\n",
      "Iteration 81, loss = 6.22651807\n",
      "Iteration 82, loss = 6.21928625\n",
      "Iteration 83, loss = 6.22062681\n",
      "Iteration 84, loss = 6.22148992\n",
      "Iteration 85, loss = 6.21950050\n",
      "Iteration 86, loss = 6.22015542\n",
      "Iteration 87, loss = 6.21678374\n",
      "Iteration 88, loss = 6.21903132\n",
      "Iteration 89, loss = 6.21966934\n",
      "Iteration 90, loss = 6.21523323\n",
      "Iteration 91, loss = 6.21198184\n",
      "Iteration 92, loss = 6.21363766\n",
      "Iteration 93, loss = 6.20993768\n",
      "Iteration 94, loss = 6.21298021\n",
      "Iteration 95, loss = 6.20557134\n",
      "Iteration 96, loss = 6.21028992\n",
      "Iteration 97, loss = 6.20901468\n",
      "Iteration 98, loss = 6.20726332\n",
      "Iteration 99, loss = 6.20720537\n",
      "Iteration 100, loss = 6.20654509\n",
      "Iteration 101, loss = 6.20812413\n",
      "Iteration 102, loss = 6.20710553\n",
      "Iteration 103, loss = 6.20244568\n",
      "Iteration 104, loss = 6.20350678\n",
      "Iteration 105, loss = 6.20217357\n",
      "Iteration 106, loss = 6.19846079\n",
      "Iteration 107, loss = 6.20120476\n",
      "Iteration 108, loss = 6.19807971\n",
      "Iteration 109, loss = 6.20299194\n",
      "Iteration 110, loss = 6.19817711\n",
      "Iteration 111, loss = 6.19657059\n",
      "Iteration 112, loss = 6.19728678\n",
      "Iteration 113, loss = 6.19732957\n",
      "Iteration 114, loss = 6.19655127\n",
      "Iteration 115, loss = 6.19294813\n",
      "Iteration 116, loss = 6.19384178\n",
      "Iteration 117, loss = 6.19050079\n",
      "Iteration 118, loss = 6.19371048\n",
      "Iteration 119, loss = 6.19006627\n",
      "Iteration 120, loss = 6.18804203\n",
      "Iteration 121, loss = 6.19360654\n",
      "Iteration 122, loss = 6.19103411\n",
      "Iteration 123, loss = 6.19192357\n",
      "Iteration 124, loss = 6.19033015\n",
      "Iteration 125, loss = 6.18739233\n",
      "Iteration 126, loss = 6.18534161\n",
      "Iteration 127, loss = 6.18875142\n",
      "Iteration 128, loss = 6.18833135\n",
      "Iteration 129, loss = 6.18531928\n",
      "Iteration 130, loss = 6.18616556\n",
      "Iteration 131, loss = 6.18548351\n",
      "Iteration 132, loss = 6.18505830\n",
      "Iteration 133, loss = 6.18390395\n",
      "Iteration 134, loss = 6.18310633\n",
      "Iteration 135, loss = 6.18732997\n",
      "Iteration 136, loss = 6.18377605\n",
      "Iteration 137, loss = 6.18611035\n",
      "Iteration 138, loss = 6.18131709\n",
      "Iteration 139, loss = 6.17794015\n",
      "Iteration 140, loss = 6.17959142\n",
      "Iteration 141, loss = 6.18062007\n",
      "Iteration 142, loss = 6.17963707\n",
      "Iteration 143, loss = 6.17339269\n",
      "Iteration 144, loss = 6.17737143\n",
      "Iteration 145, loss = 6.17634836\n",
      "Iteration 146, loss = 6.17966550\n",
      "Iteration 147, loss = 6.17575274\n",
      "Iteration 148, loss = 6.17715676\n",
      "Iteration 149, loss = 6.17200376\n",
      "Iteration 150, loss = 6.17577492\n",
      "Iteration 151, loss = 6.17484475\n",
      "Iteration 152, loss = 6.17342907\n",
      "Iteration 153, loss = 6.16964881\n",
      "Iteration 154, loss = 6.17651976\n",
      "Iteration 155, loss = 6.17184425\n",
      "Iteration 156, loss = 6.17191607\n",
      "Iteration 157, loss = 6.16791707\n",
      "Iteration 158, loss = 6.17091139\n",
      "Iteration 159, loss = 6.17009196\n",
      "Iteration 160, loss = 6.16980707\n",
      "Iteration 161, loss = 6.17161087\n",
      "Iteration 162, loss = 6.17288143\n",
      "Iteration 163, loss = 6.16620805\n",
      "Iteration 164, loss = 6.17082748\n",
      "Iteration 165, loss = 6.17022512\n",
      "Iteration 166, loss = 6.16881053\n",
      "Iteration 167, loss = 6.16659117\n",
      "Iteration 168, loss = 6.16960294\n",
      "Iteration 169, loss = 6.16589827\n",
      "Iteration 170, loss = 6.16275507\n",
      "Iteration 171, loss = 6.16673362\n",
      "Iteration 172, loss = 6.16278651\n",
      "Iteration 173, loss = 6.16224577\n",
      "Iteration 174, loss = 6.16397856\n",
      "Iteration 175, loss = 6.16505788\n",
      "Iteration 176, loss = 6.16625561\n",
      "Iteration 177, loss = 6.16596505\n",
      "Iteration 178, loss = 6.16268739\n",
      "Iteration 179, loss = 6.16122493\n",
      "Iteration 180, loss = 6.16450766\n",
      "Iteration 181, loss = 6.16023243\n",
      "Iteration 182, loss = 6.16082408\n",
      "Iteration 183, loss = 6.16152816\n",
      "Iteration 184, loss = 6.15667496\n",
      "Iteration 185, loss = 6.16024626\n",
      "Iteration 186, loss = 6.16332812\n",
      "Iteration 187, loss = 6.16011290\n",
      "Iteration 188, loss = 6.16118076\n",
      "Iteration 189, loss = 6.16495207\n",
      "Iteration 190, loss = 6.15891436\n",
      "Iteration 191, loss = 6.15864866\n",
      "Iteration 192, loss = 6.16107306\n",
      "Iteration 193, loss = 6.15829516\n",
      "Iteration 194, loss = 6.15466560\n",
      "Iteration 195, loss = 6.15561958\n",
      "Iteration 196, loss = 6.15321795\n",
      "Iteration 197, loss = 6.16255464\n",
      "Iteration 198, loss = 6.15303414\n",
      "Iteration 199, loss = 6.15490753\n",
      "Iteration 200, loss = 6.15426675\n",
      "Iteration 201, loss = 6.15490235\n",
      "Iteration 202, loss = 6.15219735\n",
      "Iteration 203, loss = 6.15173093\n",
      "Iteration 204, loss = 6.15391710\n",
      "Iteration 205, loss = 6.15521831\n",
      "Iteration 206, loss = 6.15279518\n",
      "Iteration 207, loss = 6.14935828\n",
      "Iteration 208, loss = 6.15300356\n",
      "Iteration 209, loss = 6.15010542\n",
      "Iteration 210, loss = 6.15088780\n",
      "Iteration 211, loss = 6.14827236\n",
      "Iteration 212, loss = 6.14925868\n",
      "Iteration 213, loss = 6.14930458\n",
      "Iteration 214, loss = 6.14392419\n",
      "Iteration 215, loss = 6.15230161\n",
      "Iteration 216, loss = 6.14656931\n",
      "Iteration 217, loss = 6.14945475\n",
      "Iteration 218, loss = 6.15204101\n",
      "Iteration 219, loss = 6.14771199\n",
      "Iteration 220, loss = 6.14967213\n",
      "Iteration 221, loss = 6.14665449\n",
      "Iteration 222, loss = 6.14567633\n",
      "Iteration 223, loss = 6.14580609\n",
      "Iteration 224, loss = 6.14595775\n",
      "Iteration 225, loss = 6.14712728\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/3] END activation=tanh, hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=250;, score=-13.405 total time= 5.9min\n",
      "Iteration 1, loss = 1530.38852188\n",
      "Iteration 2, loss = 105.72701890\n",
      "Iteration 3, loss = 23.69586756\n",
      "Iteration 4, loss = 10.43981894\n",
      "Iteration 5, loss = 7.63285521\n",
      "Iteration 6, loss = 7.03557146\n",
      "Iteration 7, loss = 6.86749629\n",
      "Iteration 8, loss = 6.80987014\n",
      "Iteration 9, loss = 6.77154021\n",
      "Iteration 10, loss = 6.74973854\n",
      "Iteration 11, loss = 6.72980163\n",
      "Iteration 12, loss = 6.71296652\n",
      "Iteration 13, loss = 6.69922664\n",
      "Iteration 14, loss = 6.68977677\n",
      "Iteration 15, loss = 6.67604786\n",
      "Iteration 16, loss = 6.67265124\n",
      "Iteration 17, loss = 6.65872101\n",
      "Iteration 18, loss = 6.65578167\n",
      "Iteration 19, loss = 6.64293489\n",
      "Iteration 20, loss = 6.63340727\n",
      "Iteration 21, loss = 6.62454787\n",
      "Iteration 22, loss = 6.61688766\n",
      "Iteration 23, loss = 6.61311236\n",
      "Iteration 24, loss = 6.60026673\n",
      "Iteration 25, loss = 6.59244676\n",
      "Iteration 26, loss = 6.58372596\n",
      "Iteration 27, loss = 6.57675997\n",
      "Iteration 28, loss = 6.57297003\n",
      "Iteration 29, loss = 6.55627291\n",
      "Iteration 30, loss = 6.55691359\n",
      "Iteration 31, loss = 6.54792557\n",
      "Iteration 32, loss = 6.54540257\n",
      "Iteration 33, loss = 6.53483726\n",
      "Iteration 34, loss = 6.53014303\n",
      "Iteration 35, loss = 6.52730769\n",
      "Iteration 36, loss = 6.51935227\n",
      "Iteration 37, loss = 6.51768769\n",
      "Iteration 38, loss = 6.51303131\n",
      "Iteration 39, loss = 6.50845719\n",
      "Iteration 40, loss = 6.50214553\n",
      "Iteration 41, loss = 6.50251448\n",
      "Iteration 42, loss = 6.49643549\n",
      "Iteration 43, loss = 6.49644991\n",
      "Iteration 44, loss = 6.48895498\n",
      "Iteration 45, loss = 6.49118651\n",
      "Iteration 46, loss = 6.48006142\n",
      "Iteration 47, loss = 6.48147882\n",
      "Iteration 48, loss = 6.48361695\n",
      "Iteration 49, loss = 6.47569566\n",
      "Iteration 50, loss = 6.47412355\n",
      "Iteration 51, loss = 6.46625018\n",
      "Iteration 52, loss = 6.46748247\n",
      "Iteration 53, loss = 6.46082815\n",
      "Iteration 54, loss = 6.46169058\n",
      "Iteration 55, loss = 6.46049109\n",
      "Iteration 56, loss = 6.45677320\n",
      "Iteration 57, loss = 6.45646582\n",
      "Iteration 58, loss = 6.45454977\n",
      "Iteration 59, loss = 6.45110908\n",
      "Iteration 60, loss = 6.44886703\n",
      "Iteration 61, loss = 6.44359212\n",
      "Iteration 62, loss = 6.44727132\n",
      "Iteration 63, loss = 6.44858232\n",
      "Iteration 64, loss = 6.43905361\n",
      "Iteration 65, loss = 6.43775041\n",
      "Iteration 66, loss = 6.43660679\n",
      "Iteration 67, loss = 6.43087300\n",
      "Iteration 68, loss = 6.43220675\n",
      "Iteration 69, loss = 6.43224705\n",
      "Iteration 70, loss = 6.42958461\n",
      "Iteration 71, loss = 6.42840402\n",
      "Iteration 72, loss = 6.42509772\n",
      "Iteration 73, loss = 6.42689207\n",
      "Iteration 74, loss = 6.42405450\n",
      "Iteration 75, loss = 6.42100966\n",
      "Iteration 76, loss = 6.42041220\n",
      "Iteration 77, loss = 6.42253914\n",
      "Iteration 78, loss = 6.41700489\n",
      "Iteration 79, loss = 6.41962595\n",
      "Iteration 80, loss = 6.41397766\n",
      "Iteration 81, loss = 6.41845144\n",
      "Iteration 82, loss = 6.41031907\n",
      "Iteration 83, loss = 6.41530244\n",
      "Iteration 84, loss = 6.41234694\n",
      "Iteration 85, loss = 6.40846710\n",
      "Iteration 86, loss = 6.40666162\n",
      "Iteration 87, loss = 6.40714477\n",
      "Iteration 88, loss = 6.40148607\n",
      "Iteration 89, loss = 6.39944854\n",
      "Iteration 90, loss = 6.40403866\n",
      "Iteration 91, loss = 6.39843912\n",
      "Iteration 92, loss = 6.40144059\n",
      "Iteration 93, loss = 6.39662550\n",
      "Iteration 94, loss = 6.39778079\n",
      "Iteration 95, loss = 6.39433402\n",
      "Iteration 96, loss = 6.39435089\n",
      "Iteration 97, loss = 6.39634893\n",
      "Iteration 98, loss = 6.39366421\n",
      "Iteration 99, loss = 6.39259586\n",
      "Iteration 100, loss = 6.39178994\n",
      "Iteration 101, loss = 6.39388705\n",
      "Iteration 102, loss = 6.39520187\n",
      "Iteration 103, loss = 6.39190612\n",
      "Iteration 104, loss = 6.38452758\n",
      "Iteration 105, loss = 6.38872172\n",
      "Iteration 106, loss = 6.38651876\n",
      "Iteration 107, loss = 6.38650000\n",
      "Iteration 108, loss = 6.38107730\n",
      "Iteration 109, loss = 6.38464690\n",
      "Iteration 110, loss = 6.38301863\n",
      "Iteration 111, loss = 6.38272035\n",
      "Iteration 112, loss = 6.38425480\n",
      "Iteration 113, loss = 6.38496003\n",
      "Iteration 114, loss = 6.38590408\n",
      "Iteration 115, loss = 6.38459084\n",
      "Iteration 116, loss = 6.38112865\n",
      "Iteration 117, loss = 6.37628757\n",
      "Iteration 118, loss = 6.37976522\n",
      "Iteration 119, loss = 6.37878973\n",
      "Iteration 120, loss = 6.37357991\n",
      "Iteration 121, loss = 6.38165119\n",
      "Iteration 122, loss = 6.37755807\n",
      "Iteration 123, loss = 6.37335398\n",
      "Iteration 124, loss = 6.37402779\n",
      "Iteration 125, loss = 6.37142529\n",
      "Iteration 126, loss = 6.37688155\n",
      "Iteration 127, loss = 6.37648507\n",
      "Iteration 128, loss = 6.37620351\n",
      "Iteration 129, loss = 6.37604179\n",
      "Iteration 130, loss = 6.36913301\n",
      "Iteration 131, loss = 6.36815403\n",
      "Iteration 132, loss = 6.36925286\n",
      "Iteration 133, loss = 6.36455560\n",
      "Iteration 134, loss = 6.36926167\n",
      "Iteration 135, loss = 6.36833017\n",
      "Iteration 136, loss = 6.36401346\n",
      "Iteration 137, loss = 6.36772598\n",
      "Iteration 138, loss = 6.36414968\n",
      "Iteration 139, loss = 6.36314770\n",
      "Iteration 140, loss = 6.36407009\n",
      "Iteration 141, loss = 6.36465620\n",
      "Iteration 142, loss = 6.36429207\n",
      "Iteration 143, loss = 6.36111828\n",
      "Iteration 144, loss = 6.36740733\n",
      "Iteration 145, loss = 6.36052115\n",
      "Iteration 146, loss = 6.36176090\n",
      "Iteration 147, loss = 6.36134220\n",
      "Iteration 148, loss = 6.36124279\n",
      "Iteration 149, loss = 6.35913436\n",
      "Iteration 150, loss = 6.35467441\n",
      "Iteration 151, loss = 6.35846910\n",
      "Iteration 152, loss = 6.35450301\n",
      "Iteration 153, loss = 6.36026065\n",
      "Iteration 154, loss = 6.35629984\n",
      "Iteration 155, loss = 6.35505779\n",
      "Iteration 156, loss = 6.35458880\n",
      "Iteration 157, loss = 6.35204358\n",
      "Iteration 158, loss = 6.35529355\n",
      "Iteration 159, loss = 6.35490354\n",
      "Iteration 160, loss = 6.35151900\n",
      "Iteration 161, loss = 6.35127342\n",
      "Iteration 162, loss = 6.35537309\n",
      "Iteration 163, loss = 6.34841514\n",
      "Iteration 164, loss = 6.34685385\n",
      "Iteration 165, loss = 6.35214875\n",
      "Iteration 166, loss = 6.35268404\n",
      "Iteration 167, loss = 6.34715053\n",
      "Iteration 168, loss = 6.34556577\n",
      "Iteration 169, loss = 6.34745100\n",
      "Iteration 170, loss = 6.34785831\n",
      "Iteration 171, loss = 6.34446944\n",
      "Iteration 172, loss = 6.34945541\n",
      "Iteration 173, loss = 6.34472855\n",
      "Iteration 174, loss = 6.34480254\n",
      "Iteration 175, loss = 6.34382212\n",
      "Iteration 176, loss = 6.34548014\n",
      "Iteration 177, loss = 6.34405809\n",
      "Iteration 178, loss = 6.33939983\n",
      "Iteration 179, loss = 6.33762025\n",
      "Iteration 180, loss = 6.33724277\n",
      "Iteration 181, loss = 6.34002094\n",
      "Iteration 182, loss = 6.33559314\n",
      "Iteration 183, loss = 6.33996785\n",
      "Iteration 184, loss = 6.33519890\n",
      "Iteration 185, loss = 6.33266140\n",
      "Iteration 186, loss = 6.33687147\n",
      "Iteration 187, loss = 6.33516837\n",
      "Iteration 188, loss = 6.33475262\n",
      "Iteration 189, loss = 6.33538464\n",
      "Iteration 190, loss = 6.32982662\n",
      "Iteration 191, loss = 6.33570157\n",
      "Iteration 192, loss = 6.33297095\n",
      "Iteration 193, loss = 6.33093158\n",
      "Iteration 194, loss = 6.33215355\n",
      "Iteration 195, loss = 6.32612660\n",
      "Iteration 196, loss = 6.32689261\n",
      "Iteration 197, loss = 6.32626180\n",
      "Iteration 198, loss = 6.32711463\n",
      "Iteration 199, loss = 6.32669681\n",
      "Iteration 200, loss = 6.32604577\n",
      "Iteration 201, loss = 6.32697520\n",
      "Iteration 202, loss = 6.32599540\n",
      "Iteration 203, loss = 6.32141507\n",
      "Iteration 204, loss = 6.32229485\n",
      "Iteration 205, loss = 6.32383370\n",
      "Iteration 206, loss = 6.31514964\n",
      "Iteration 207, loss = 6.31946394\n",
      "Iteration 208, loss = 6.32195652\n",
      "Iteration 209, loss = 6.32052966\n",
      "Iteration 210, loss = 6.31757170\n",
      "Iteration 211, loss = 6.32249817\n",
      "Iteration 212, loss = 6.31625299\n",
      "Iteration 213, loss = 6.31627703\n",
      "Iteration 214, loss = 6.31667100\n",
      "Iteration 215, loss = 6.31684589\n",
      "Iteration 216, loss = 6.31383511\n",
      "Iteration 217, loss = 6.31451830\n",
      "Iteration 218, loss = 6.31974689\n",
      "Iteration 219, loss = 6.31297127\n",
      "Iteration 220, loss = 6.31400707\n",
      "Iteration 221, loss = 6.31019166\n",
      "Iteration 222, loss = 6.31269052\n",
      "Iteration 223, loss = 6.30979388\n",
      "Iteration 224, loss = 6.31162446\n",
      "Iteration 225, loss = 6.31259201\n",
      "Iteration 226, loss = 6.30785545\n",
      "Iteration 227, loss = 6.30612802\n",
      "Iteration 228, loss = 6.30772672\n",
      "Iteration 229, loss = 6.30579267\n",
      "Iteration 230, loss = 6.30919993\n",
      "Iteration 231, loss = 6.30347704\n",
      "Iteration 232, loss = 6.30778959\n",
      "Iteration 233, loss = 6.30464354\n",
      "Iteration 234, loss = 6.30710094\n",
      "Iteration 235, loss = 6.30473954\n",
      "Iteration 236, loss = 6.30534778\n",
      "Iteration 237, loss = 6.30479854\n",
      "Iteration 238, loss = 6.30628944\n",
      "Iteration 239, loss = 6.30353093\n",
      "Iteration 240, loss = 6.30014702\n",
      "Iteration 241, loss = 6.29555316\n",
      "Iteration 242, loss = 6.30406621\n",
      "Iteration 243, loss = 6.30104901\n",
      "Iteration 244, loss = 6.29705779\n",
      "Iteration 245, loss = 6.29967519\n",
      "Iteration 246, loss = 6.30395745\n",
      "Iteration 247, loss = 6.30077009\n",
      "Iteration 248, loss = 6.30199588\n",
      "Iteration 249, loss = 6.29652618\n",
      "Iteration 250, loss = 6.29855415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Downloads\\IMF\\Modulos\\Aun por resolver\\TFM\\TFM\\calories\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=tanh, hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=250;, score=-12.372 total time= 6.4min\n",
      "Iteration 1, loss = 1531.34136176\n",
      "Iteration 2, loss = 105.76070099\n",
      "Iteration 3, loss = 23.70730957\n",
      "Iteration 4, loss = 10.37098841\n",
      "Iteration 5, loss = 7.53123705\n",
      "Iteration 6, loss = 6.93578595\n",
      "Iteration 7, loss = 6.77389488\n",
      "Iteration 8, loss = 6.71667487\n",
      "Iteration 9, loss = 6.68340046\n",
      "Iteration 10, loss = 6.65356190\n",
      "Iteration 11, loss = 6.63908505\n",
      "Iteration 12, loss = 6.62298880\n",
      "Iteration 13, loss = 6.60920330\n",
      "Iteration 14, loss = 6.60026477\n",
      "Iteration 15, loss = 6.58545959\n",
      "Iteration 16, loss = 6.57989525\n",
      "Iteration 17, loss = 6.57253926\n",
      "Iteration 18, loss = 6.56884851\n",
      "Iteration 19, loss = 6.56141874\n",
      "Iteration 20, loss = 6.55402555\n",
      "Iteration 21, loss = 6.55003687\n",
      "Iteration 22, loss = 6.53772104\n",
      "Iteration 23, loss = 6.53420250\n",
      "Iteration 24, loss = 6.52779573\n",
      "Iteration 25, loss = 6.52411013\n",
      "Iteration 26, loss = 6.51473133\n",
      "Iteration 27, loss = 6.50802236\n",
      "Iteration 28, loss = 6.50132181\n",
      "Iteration 29, loss = 6.49248655\n",
      "Iteration 30, loss = 6.48661694\n",
      "Iteration 31, loss = 6.48180080\n",
      "Iteration 32, loss = 6.47889616\n",
      "Iteration 33, loss = 6.47252423\n",
      "Iteration 34, loss = 6.46353481\n",
      "Iteration 35, loss = 6.45889355\n",
      "Iteration 36, loss = 6.45067684\n",
      "Iteration 37, loss = 6.44899962\n",
      "Iteration 38, loss = 6.44288518\n",
      "Iteration 39, loss = 6.44163762\n",
      "Iteration 40, loss = 6.43299004\n",
      "Iteration 41, loss = 6.43204087\n",
      "Iteration 42, loss = 6.42743068\n",
      "Iteration 43, loss = 6.42441383\n",
      "Iteration 44, loss = 6.42360807\n",
      "Iteration 45, loss = 6.41525477\n",
      "Iteration 46, loss = 6.41778956\n",
      "Iteration 47, loss = 6.40707072\n",
      "Iteration 48, loss = 6.41056289\n",
      "Iteration 49, loss = 6.40385301\n",
      "Iteration 50, loss = 6.40207770\n",
      "Iteration 51, loss = 6.39978627\n",
      "Iteration 52, loss = 6.39152461\n",
      "Iteration 53, loss = 6.39379267\n",
      "Iteration 54, loss = 6.39680987\n",
      "Iteration 55, loss = 6.38926057\n",
      "Iteration 56, loss = 6.38385718\n",
      "Iteration 57, loss = 6.38351480\n",
      "Iteration 58, loss = 6.38032120\n",
      "Iteration 59, loss = 6.38040682\n",
      "Iteration 60, loss = 6.37833074\n",
      "Iteration 61, loss = 6.37486164\n",
      "Iteration 62, loss = 6.37046185\n",
      "Iteration 63, loss = 6.37394193\n",
      "Iteration 64, loss = 6.37311748\n",
      "Iteration 65, loss = 6.36598828\n",
      "Iteration 66, loss = 6.36768207\n",
      "Iteration 67, loss = 6.36377032\n",
      "Iteration 68, loss = 6.36318109\n",
      "Iteration 69, loss = 6.36168362\n",
      "Iteration 70, loss = 6.36120031\n",
      "Iteration 71, loss = 6.35670066\n",
      "Iteration 72, loss = 6.35428735\n",
      "Iteration 73, loss = 6.35770715\n",
      "Iteration 74, loss = 6.35368676\n",
      "Iteration 75, loss = 6.34807422\n",
      "Iteration 76, loss = 6.35563624\n",
      "Iteration 77, loss = 6.34990615\n",
      "Iteration 78, loss = 6.34836585\n",
      "Iteration 79, loss = 6.34531644\n",
      "Iteration 80, loss = 6.34970132\n",
      "Iteration 81, loss = 6.34431088\n",
      "Iteration 82, loss = 6.34258935\n",
      "Iteration 83, loss = 6.34074873\n",
      "Iteration 84, loss = 6.34388012\n",
      "Iteration 85, loss = 6.34193905\n",
      "Iteration 86, loss = 6.33506937\n",
      "Iteration 87, loss = 6.33785546\n",
      "Iteration 88, loss = 6.33404074\n",
      "Iteration 89, loss = 6.33275858\n",
      "Iteration 90, loss = 6.32705676\n",
      "Iteration 91, loss = 6.33235718\n",
      "Iteration 92, loss = 6.33576418\n",
      "Iteration 93, loss = 6.32961019\n",
      "Iteration 94, loss = 6.32541849\n",
      "Iteration 95, loss = 6.33219947\n",
      "Iteration 96, loss = 6.32161126\n",
      "Iteration 97, loss = 6.32952304\n",
      "Iteration 98, loss = 6.32623301\n",
      "Iteration 99, loss = 6.32145262\n",
      "Iteration 100, loss = 6.32342874\n",
      "Iteration 101, loss = 6.32689526\n",
      "Iteration 102, loss = 6.32523998\n",
      "Iteration 103, loss = 6.32417586\n",
      "Iteration 104, loss = 6.31640981\n",
      "Iteration 105, loss = 6.32170693\n",
      "Iteration 106, loss = 6.31386675\n",
      "Iteration 107, loss = 6.31590520\n",
      "Iteration 108, loss = 6.31728457\n",
      "Iteration 109, loss = 6.31497894\n",
      "Iteration 110, loss = 6.31422714\n",
      "Iteration 111, loss = 6.31324242\n",
      "Iteration 112, loss = 6.31020983\n",
      "Iteration 113, loss = 6.31155937\n",
      "Iteration 114, loss = 6.31485950\n",
      "Iteration 115, loss = 6.31111132\n",
      "Iteration 116, loss = 6.30637564\n",
      "Iteration 117, loss = 6.30631543\n",
      "Iteration 118, loss = 6.30609656\n",
      "Iteration 119, loss = 6.30884394\n",
      "Iteration 120, loss = 6.30269255\n",
      "Iteration 121, loss = 6.31084893\n",
      "Iteration 122, loss = 6.30620731\n",
      "Iteration 123, loss = 6.29777736\n",
      "Iteration 124, loss = 6.30303377\n",
      "Iteration 125, loss = 6.30238578\n",
      "Iteration 126, loss = 6.30404655\n",
      "Iteration 127, loss = 6.30057478\n",
      "Iteration 128, loss = 6.30037970\n",
      "Iteration 129, loss = 6.29735518\n",
      "Iteration 130, loss = 6.30170136\n",
      "Iteration 131, loss = 6.29799748\n",
      "Iteration 132, loss = 6.29548201\n",
      "Iteration 133, loss = 6.29897781\n",
      "Iteration 134, loss = 6.29513412\n",
      "Iteration 135, loss = 6.29716467\n",
      "Iteration 136, loss = 6.29514478\n",
      "Iteration 137, loss = 6.29545357\n",
      "Iteration 138, loss = 6.29382759\n",
      "Iteration 139, loss = 6.29161612\n",
      "Iteration 140, loss = 6.29308727\n",
      "Iteration 141, loss = 6.29147265\n",
      "Iteration 142, loss = 6.28569319\n",
      "Iteration 143, loss = 6.28953176\n",
      "Iteration 144, loss = 6.28782823\n",
      "Iteration 145, loss = 6.28307287\n",
      "Iteration 146, loss = 6.28863852\n",
      "Iteration 147, loss = 6.28556725\n",
      "Iteration 148, loss = 6.28340193\n",
      "Iteration 149, loss = 6.28559657\n",
      "Iteration 150, loss = 6.27884477\n",
      "Iteration 151, loss = 6.27944218\n",
      "Iteration 152, loss = 6.28241965\n",
      "Iteration 153, loss = 6.28071675\n",
      "Iteration 154, loss = 6.27944678\n",
      "Iteration 155, loss = 6.27526645\n",
      "Iteration 156, loss = 6.27386579\n",
      "Iteration 157, loss = 6.28205386\n",
      "Iteration 158, loss = 6.27440143\n",
      "Iteration 159, loss = 6.27838407\n",
      "Iteration 160, loss = 6.27544237\n",
      "Iteration 161, loss = 6.27366324\n",
      "Iteration 162, loss = 6.26973192\n",
      "Iteration 163, loss = 6.27099934\n",
      "Iteration 164, loss = 6.26782938\n",
      "Iteration 165, loss = 6.27240775\n",
      "Iteration 166, loss = 6.26845843\n",
      "Iteration 167, loss = 6.27310603\n",
      "Iteration 168, loss = 6.26854985\n",
      "Iteration 169, loss = 6.27013328\n",
      "Iteration 170, loss = 6.26415339\n",
      "Iteration 171, loss = 6.26068351\n",
      "Iteration 172, loss = 6.26499756\n",
      "Iteration 173, loss = 6.26566868\n",
      "Iteration 174, loss = 6.25871715\n",
      "Iteration 175, loss = 6.26236981\n",
      "Iteration 176, loss = 6.25963051\n",
      "Iteration 177, loss = 6.26019196\n",
      "Iteration 178, loss = 6.25878723\n",
      "Iteration 179, loss = 6.25248984\n",
      "Iteration 180, loss = 6.26130676\n",
      "Iteration 181, loss = 6.25864545\n",
      "Iteration 182, loss = 6.25854512\n",
      "Iteration 183, loss = 6.25647205\n",
      "Iteration 184, loss = 6.25699222\n",
      "Iteration 185, loss = 6.25257977\n",
      "Iteration 186, loss = 6.25076453\n",
      "Iteration 187, loss = 6.25503234\n",
      "Iteration 188, loss = 6.25325602\n",
      "Iteration 189, loss = 6.25100610\n",
      "Iteration 190, loss = 6.25066470\n",
      "Iteration 191, loss = 6.25403781\n",
      "Iteration 192, loss = 6.25049983\n",
      "Iteration 193, loss = 6.24894337\n",
      "Iteration 194, loss = 6.24817097\n",
      "Iteration 195, loss = 6.24688882\n",
      "Iteration 196, loss = 6.24687676\n",
      "Iteration 197, loss = 6.24778158\n",
      "Iteration 198, loss = 6.24589935\n",
      "Iteration 199, loss = 6.24228917\n",
      "Iteration 200, loss = 6.24494467\n",
      "Iteration 201, loss = 6.24523054\n",
      "Iteration 202, loss = 6.23867884\n",
      "Iteration 203, loss = 6.24363826\n",
      "Iteration 204, loss = 6.23738973\n",
      "Iteration 205, loss = 6.23975612\n",
      "Iteration 206, loss = 6.23990234\n",
      "Iteration 207, loss = 6.23945711\n",
      "Iteration 208, loss = 6.24034472\n",
      "Iteration 209, loss = 6.23778541\n",
      "Iteration 210, loss = 6.24243251\n",
      "Iteration 211, loss = 6.23996753\n",
      "Iteration 212, loss = 6.23710605\n",
      "Iteration 213, loss = 6.23816388\n",
      "Iteration 214, loss = 6.23817540\n",
      "Iteration 215, loss = 6.23486490\n",
      "Iteration 216, loss = 6.23560534\n",
      "Iteration 217, loss = 6.23301865\n",
      "Iteration 218, loss = 6.23350653\n",
      "Iteration 219, loss = 6.23049547\n",
      "Iteration 220, loss = 6.23467671\n",
      "Iteration 221, loss = 6.22720538\n",
      "Iteration 222, loss = 6.23093990\n",
      "Iteration 223, loss = 6.23184719\n",
      "Iteration 224, loss = 6.23053875\n",
      "Iteration 225, loss = 6.22900005\n",
      "Iteration 226, loss = 6.22633145\n",
      "Iteration 227, loss = 6.23107104\n",
      "Iteration 228, loss = 6.22931998\n",
      "Iteration 229, loss = 6.22461402\n",
      "Iteration 230, loss = 6.22962078\n",
      "Iteration 231, loss = 6.22257433\n",
      "Iteration 232, loss = 6.22868416\n",
      "Iteration 233, loss = 6.22424374\n",
      "Iteration 234, loss = 6.22735725\n",
      "Iteration 235, loss = 6.22364791\n",
      "Iteration 236, loss = 6.22618038\n",
      "Iteration 237, loss = 6.22566094\n",
      "Iteration 238, loss = 6.22338393\n",
      "Iteration 239, loss = 6.22101782\n",
      "Iteration 240, loss = 6.22297992\n",
      "Iteration 241, loss = 6.22087237\n",
      "Iteration 242, loss = 6.22356646\n",
      "Iteration 243, loss = 6.22088364\n",
      "Iteration 244, loss = 6.21900446\n",
      "Iteration 245, loss = 6.21986834\n",
      "Iteration 246, loss = 6.22143016\n",
      "Iteration 247, loss = 6.21900038\n",
      "Iteration 248, loss = 6.22259110\n",
      "Iteration 249, loss = 6.21442511\n",
      "Iteration 250, loss = 6.22073786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Downloads\\IMF\\Modulos\\Aun por resolver\\TFM\\TFM\\calories\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=tanh, hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=250;, score=-12.701 total time= 6.5min\n",
      "Iteration 1, loss = 993.64399520\n",
      "Iteration 2, loss = 22.92647721\n",
      "Iteration 3, loss = 10.11119647\n",
      "Iteration 4, loss = 7.87937071\n",
      "Iteration 5, loss = 7.21138760\n",
      "Iteration 6, loss = 6.94725304\n",
      "Iteration 7, loss = 6.82956457\n",
      "Iteration 8, loss = 6.75802048\n",
      "Iteration 9, loss = 6.70760562\n",
      "Iteration 10, loss = 6.67963887\n",
      "Iteration 11, loss = 6.65099879\n",
      "Iteration 12, loss = 6.63017390\n",
      "Iteration 13, loss = 6.61132295\n",
      "Iteration 14, loss = 6.60163602\n",
      "Iteration 15, loss = 6.59202078\n",
      "Iteration 16, loss = 6.58169392\n",
      "Iteration 17, loss = 6.57259398\n",
      "Iteration 18, loss = 6.56888057\n",
      "Iteration 19, loss = 6.55921320\n",
      "Iteration 20, loss = 6.55160458\n",
      "Iteration 21, loss = 6.54398454\n",
      "Iteration 22, loss = 6.54140416\n",
      "Iteration 23, loss = 6.53468463\n",
      "Iteration 24, loss = 6.53171284\n",
      "Iteration 25, loss = 6.52215191\n",
      "Iteration 26, loss = 6.51917040\n",
      "Iteration 27, loss = 6.51523495\n",
      "Iteration 28, loss = 6.51105398\n",
      "Iteration 29, loss = 6.50338294\n",
      "Iteration 30, loss = 6.50309890\n",
      "Iteration 31, loss = 6.49925229\n",
      "Iteration 32, loss = 6.49567304\n",
      "Iteration 33, loss = 6.49398811\n",
      "Iteration 34, loss = 6.49164999\n",
      "Iteration 35, loss = 6.49595522\n",
      "Iteration 36, loss = 6.48990024\n",
      "Iteration 37, loss = 6.48325716\n",
      "Iteration 38, loss = 6.48130508\n",
      "Iteration 39, loss = 6.48400766\n",
      "Iteration 40, loss = 6.47991152\n",
      "Iteration 41, loss = 6.47986641\n",
      "Iteration 42, loss = 6.47923733\n",
      "Iteration 43, loss = 6.47309854\n",
      "Iteration 44, loss = 6.47892680\n",
      "Iteration 45, loss = 6.47112548\n",
      "Iteration 46, loss = 6.46944862\n",
      "Iteration 47, loss = 6.47171441\n",
      "Iteration 48, loss = 6.47049953\n",
      "Iteration 49, loss = 6.47091240\n",
      "Iteration 50, loss = 6.46724037\n",
      "Iteration 51, loss = 6.46319288\n",
      "Iteration 52, loss = 6.46613823\n",
      "Iteration 53, loss = 6.46634336\n",
      "Iteration 54, loss = 6.46401085\n",
      "Iteration 55, loss = 6.46314849\n",
      "Iteration 56, loss = 6.46457959\n",
      "Iteration 57, loss = 6.45763222\n",
      "Iteration 58, loss = 6.45665424\n",
      "Iteration 59, loss = 6.45584425\n",
      "Iteration 60, loss = 6.45378403\n",
      "Iteration 61, loss = 6.45009102\n",
      "Iteration 62, loss = 6.45127570\n",
      "Iteration 63, loss = 6.45287587\n",
      "Iteration 64, loss = 6.44843069\n",
      "Iteration 65, loss = 6.45093180\n",
      "Iteration 66, loss = 6.45107417\n",
      "Iteration 67, loss = 6.45170303\n",
      "Iteration 68, loss = 6.44598993\n",
      "Iteration 69, loss = 6.44640164\n",
      "Iteration 70, loss = 6.44553390\n",
      "Iteration 71, loss = 6.44606921\n",
      "Iteration 72, loss = 6.44168917\n",
      "Iteration 73, loss = 6.44042651\n",
      "Iteration 74, loss = 6.44582541\n",
      "Iteration 75, loss = 6.44362692\n",
      "Iteration 76, loss = 6.44347151\n",
      "Iteration 77, loss = 6.44714629\n",
      "Iteration 78, loss = 6.44276974\n",
      "Iteration 79, loss = 6.44041010\n",
      "Iteration 80, loss = 6.44098809\n",
      "Iteration 81, loss = 6.43414904\n",
      "Iteration 82, loss = 6.43691368\n",
      "Iteration 83, loss = 6.44246857\n",
      "Iteration 84, loss = 6.43338148\n",
      "Iteration 85, loss = 6.43914931\n",
      "Iteration 86, loss = 6.43241204\n",
      "Iteration 87, loss = 6.43378630\n",
      "Iteration 88, loss = 6.43188631\n",
      "Iteration 89, loss = 6.43170765\n",
      "Iteration 90, loss = 6.43622601\n",
      "Iteration 91, loss = 6.42917147\n",
      "Iteration 92, loss = 6.42857365\n",
      "Iteration 93, loss = 6.42818674\n",
      "Iteration 94, loss = 6.42837895\n",
      "Iteration 95, loss = 6.42478601\n",
      "Iteration 96, loss = 6.42526434\n",
      "Iteration 97, loss = 6.42534058\n",
      "Iteration 98, loss = 6.42548070\n",
      "Iteration 99, loss = 6.42611775\n",
      "Iteration 100, loss = 6.42256512\n",
      "Iteration 101, loss = 6.42466938\n",
      "Iteration 102, loss = 6.41839117\n",
      "Iteration 103, loss = 6.41968314\n",
      "Iteration 104, loss = 6.42363559\n",
      "Iteration 105, loss = 6.41987889\n",
      "Iteration 106, loss = 6.41825351\n",
      "Iteration 107, loss = 6.41802947\n",
      "Iteration 108, loss = 6.41383887\n",
      "Iteration 109, loss = 6.41547891\n",
      "Iteration 110, loss = 6.41436134\n",
      "Iteration 111, loss = 6.42016746\n",
      "Iteration 112, loss = 6.41134282\n",
      "Iteration 113, loss = 6.41221415\n",
      "Iteration 114, loss = 6.41455895\n",
      "Iteration 115, loss = 6.41605061\n",
      "Iteration 116, loss = 6.41346035\n",
      "Iteration 117, loss = 6.41285457\n",
      "Iteration 118, loss = 6.40815464\n",
      "Iteration 119, loss = 6.41545205\n",
      "Iteration 120, loss = 6.40996542\n",
      "Iteration 121, loss = 6.40722166\n",
      "Iteration 122, loss = 6.41430611\n",
      "Iteration 123, loss = 6.41041598\n",
      "Iteration 124, loss = 6.40568963\n",
      "Iteration 125, loss = 6.40679899\n",
      "Iteration 126, loss = 6.40515638\n",
      "Iteration 127, loss = 6.40541256\n",
      "Iteration 128, loss = 6.40337376\n",
      "Iteration 129, loss = 6.40906087\n",
      "Iteration 130, loss = 6.40953893\n",
      "Iteration 131, loss = 6.40201102\n",
      "Iteration 132, loss = 6.40408194\n",
      "Iteration 133, loss = 6.39636315\n",
      "Iteration 134, loss = 6.40437241\n",
      "Iteration 135, loss = 6.40274848\n",
      "Iteration 136, loss = 6.40432570\n",
      "Iteration 137, loss = 6.40246114\n",
      "Iteration 138, loss = 6.40285933\n",
      "Iteration 139, loss = 6.40158305\n",
      "Iteration 140, loss = 6.39848047\n",
      "Iteration 141, loss = 6.40090393\n",
      "Iteration 142, loss = 6.39790632\n",
      "Iteration 143, loss = 6.39442772\n",
      "Iteration 144, loss = 6.40016873\n",
      "Iteration 145, loss = 6.40008487\n",
      "Iteration 146, loss = 6.39870527\n",
      "Iteration 147, loss = 6.39858379\n",
      "Iteration 148, loss = 6.39844742\n",
      "Iteration 149, loss = 6.39664383\n",
      "Iteration 150, loss = 6.39534989\n",
      "Iteration 151, loss = 6.39670427\n",
      "Iteration 152, loss = 6.39694834\n",
      "Iteration 153, loss = 6.39400541\n",
      "Iteration 154, loss = 6.39198036\n",
      "Iteration 155, loss = 6.39707637\n",
      "Iteration 156, loss = 6.39258227\n",
      "Iteration 157, loss = 6.39278303\n",
      "Iteration 158, loss = 6.39274424\n",
      "Iteration 159, loss = 6.39432307\n",
      "Iteration 160, loss = 6.39180825\n",
      "Iteration 161, loss = 6.39142861\n",
      "Iteration 162, loss = 6.38762674\n",
      "Iteration 163, loss = 6.39015849\n",
      "Iteration 164, loss = 6.39016884\n",
      "Iteration 165, loss = 6.39234300\n",
      "Iteration 166, loss = 6.38784145\n",
      "Iteration 167, loss = 6.39295573\n",
      "Iteration 168, loss = 6.38834119\n",
      "Iteration 169, loss = 6.39196928\n",
      "Iteration 170, loss = 6.39047148\n",
      "Iteration 171, loss = 6.38976173\n",
      "Iteration 172, loss = 6.38650827\n",
      "Iteration 173, loss = 6.39185430\n",
      "Iteration 174, loss = 6.38634603\n",
      "Iteration 175, loss = 6.38707049\n",
      "Iteration 176, loss = 6.38513134\n",
      "Iteration 177, loss = 6.38855816\n",
      "Iteration 178, loss = 6.39233464\n",
      "Iteration 179, loss = 6.38472031\n",
      "Iteration 180, loss = 6.39183584\n",
      "Iteration 181, loss = 6.38562163\n",
      "Iteration 182, loss = 6.38180044\n",
      "Iteration 183, loss = 6.39037389\n",
      "Iteration 184, loss = 6.38304259\n",
      "Iteration 185, loss = 6.38028515\n",
      "Iteration 186, loss = 6.38281340\n",
      "Iteration 187, loss = 6.38175152\n",
      "Iteration 188, loss = 6.38091142\n",
      "Iteration 189, loss = 6.37963645\n",
      "Iteration 190, loss = 6.38303840\n",
      "Iteration 191, loss = 6.38321538\n",
      "Iteration 192, loss = 6.37997000\n",
      "Iteration 193, loss = 6.37879017\n",
      "Iteration 194, loss = 6.37698994\n",
      "Iteration 195, loss = 6.37800303\n",
      "Iteration 196, loss = 6.38162644\n",
      "Iteration 197, loss = 6.37704076\n",
      "Iteration 198, loss = 6.38038217\n",
      "Iteration 199, loss = 6.38030619\n",
      "Iteration 200, loss = 6.37556737\n",
      "Iteration 201, loss = 6.37690694\n",
      "Iteration 202, loss = 6.37679801\n",
      "Iteration 203, loss = 6.38143280\n",
      "Iteration 204, loss = 6.37840130\n",
      "Iteration 205, loss = 6.37654367\n",
      "Iteration 206, loss = 6.37544572\n",
      "Iteration 207, loss = 6.37621143\n",
      "Iteration 208, loss = 6.37546405\n",
      "Iteration 209, loss = 6.37686909\n",
      "Iteration 210, loss = 6.37321448\n",
      "Iteration 211, loss = 6.38273884\n",
      "Iteration 212, loss = 6.37821006\n",
      "Iteration 213, loss = 6.37486740\n",
      "Iteration 214, loss = 6.37404525\n",
      "Iteration 215, loss = 6.37171908\n",
      "Iteration 216, loss = 6.37453257\n",
      "Iteration 217, loss = 6.37222742\n",
      "Iteration 218, loss = 6.37282643\n",
      "Iteration 219, loss = 6.37046637\n",
      "Iteration 220, loss = 6.37245778\n",
      "Iteration 221, loss = 6.37132519\n",
      "Iteration 222, loss = 6.37241806\n",
      "Iteration 223, loss = 6.37275908\n",
      "Iteration 224, loss = 6.37215389\n",
      "Iteration 225, loss = 6.37275611\n",
      "Iteration 226, loss = 6.37050656\n",
      "Iteration 227, loss = 6.37455486\n",
      "Iteration 228, loss = 6.36855973\n",
      "Iteration 229, loss = 6.37477530\n",
      "Iteration 230, loss = 6.37015459\n",
      "Iteration 231, loss = 6.36884686\n",
      "Iteration 232, loss = 6.36835069\n",
      "Iteration 233, loss = 6.37447408\n",
      "Iteration 234, loss = 6.37223516\n",
      "Iteration 235, loss = 6.36880623\n",
      "Iteration 236, loss = 6.36873860\n",
      "Iteration 237, loss = 6.36847241\n",
      "Iteration 238, loss = 6.36560081\n",
      "Iteration 239, loss = 6.37447335\n",
      "Iteration 240, loss = 6.37078463\n",
      "Iteration 241, loss = 6.36651628\n",
      "Iteration 242, loss = 6.36979401\n",
      "Iteration 243, loss = 6.37000840\n",
      "Iteration 244, loss = 6.36762429\n",
      "Iteration 245, loss = 6.36685917\n",
      "Iteration 246, loss = 6.36894332\n",
      "Iteration 247, loss = 6.36792386\n",
      "Iteration 248, loss = 6.36847153\n",
      "Iteration 249, loss = 6.36549969\n",
      "Iteration 250, loss = 6.36769782\n",
      "[CV 1/3] END activation=relu, hidden_layer_sizes=(50,), learning_rate_init=0.001, max_iter=250;, score=-13.755 total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Downloads\\IMF\\Modulos\\Aun por resolver\\TFM\\TFM\\calories\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 995.10336294\n",
      "Iteration 2, loss = 23.23087698\n",
      "Iteration 3, loss = 10.52046785\n",
      "Iteration 4, loss = 8.19045859\n",
      "Iteration 5, loss = 7.51494874\n",
      "Iteration 6, loss = 7.25318657\n",
      "Iteration 7, loss = 7.13557344\n",
      "Iteration 8, loss = 7.06634294\n",
      "Iteration 9, loss = 7.01220841\n",
      "Iteration 10, loss = 6.98741986\n",
      "Iteration 11, loss = 6.95905001\n",
      "Iteration 12, loss = 6.94096671\n",
      "Iteration 13, loss = 6.92315111\n",
      "Iteration 14, loss = 6.91152933\n",
      "Iteration 15, loss = 6.89333273\n",
      "Iteration 16, loss = 6.88776462\n",
      "Iteration 17, loss = 6.87839710\n",
      "Iteration 18, loss = 6.87282705\n",
      "Iteration 19, loss = 6.86260079\n",
      "Iteration 20, loss = 6.85667062\n",
      "Iteration 21, loss = 6.85303821\n",
      "Iteration 22, loss = 6.84482809\n",
      "Iteration 23, loss = 6.83985093\n",
      "Iteration 24, loss = 6.83828000\n",
      "Iteration 25, loss = 6.83128011\n",
      "Iteration 26, loss = 6.82666122\n",
      "Iteration 27, loss = 6.82096473\n",
      "Iteration 28, loss = 6.81432997\n",
      "Iteration 29, loss = 6.80954035\n",
      "Iteration 30, loss = 6.80600938\n",
      "Iteration 31, loss = 6.79682470\n",
      "Iteration 32, loss = 6.79478947\n",
      "Iteration 33, loss = 6.79108026\n",
      "Iteration 34, loss = 6.79024332\n",
      "Iteration 35, loss = 6.78750540\n",
      "Iteration 36, loss = 6.78427384\n",
      "Iteration 37, loss = 6.78038058\n",
      "Iteration 38, loss = 6.77518869\n",
      "Iteration 39, loss = 6.77973120\n",
      "Iteration 40, loss = 6.76894327\n",
      "Iteration 41, loss = 6.76889409\n",
      "Iteration 42, loss = 6.76419498\n",
      "Iteration 43, loss = 6.76179654\n",
      "Iteration 44, loss = 6.75945746\n",
      "Iteration 45, loss = 6.75618341\n",
      "Iteration 46, loss = 6.75669773\n",
      "Iteration 47, loss = 6.74790050\n",
      "Iteration 48, loss = 6.74810091\n",
      "Iteration 49, loss = 6.75032548\n",
      "Iteration 50, loss = 6.74341844\n",
      "Iteration 51, loss = 6.74350553\n",
      "Iteration 52, loss = 6.73923890\n",
      "Iteration 53, loss = 6.73499066\n",
      "Iteration 54, loss = 6.73545304\n",
      "Iteration 55, loss = 6.73625969\n",
      "Iteration 56, loss = 6.73802353\n",
      "Iteration 57, loss = 6.72994847\n",
      "Iteration 58, loss = 6.72955618\n",
      "Iteration 59, loss = 6.73079838\n",
      "Iteration 60, loss = 6.72645335\n",
      "Iteration 61, loss = 6.72445301\n",
      "Iteration 62, loss = 6.72233058\n",
      "Iteration 63, loss = 6.72304571\n",
      "Iteration 64, loss = 6.71930786\n",
      "Iteration 65, loss = 6.72048905\n",
      "Iteration 66, loss = 6.71466229\n",
      "Iteration 67, loss = 6.71683495\n",
      "Iteration 68, loss = 6.71279841\n",
      "Iteration 69, loss = 6.71503035\n",
      "Iteration 70, loss = 6.70968738\n",
      "Iteration 71, loss = 6.70770201\n",
      "Iteration 72, loss = 6.70524615\n",
      "Iteration 73, loss = 6.70715102\n",
      "Iteration 74, loss = 6.70512932\n",
      "Iteration 75, loss = 6.70555427\n",
      "Iteration 76, loss = 6.70666257\n",
      "Iteration 77, loss = 6.70325470\n",
      "Iteration 78, loss = 6.70697389\n",
      "Iteration 79, loss = 6.70138861\n",
      "Iteration 80, loss = 6.69753624\n",
      "Iteration 81, loss = 6.69302185\n",
      "Iteration 82, loss = 6.69560773\n",
      "Iteration 83, loss = 6.70030093\n",
      "Iteration 84, loss = 6.69962168\n",
      "Iteration 85, loss = 6.69768170\n",
      "Iteration 86, loss = 6.69166970\n",
      "Iteration 87, loss = 6.69678304\n",
      "Iteration 88, loss = 6.69273704\n",
      "Iteration 89, loss = 6.68785658\n",
      "Iteration 90, loss = 6.69052878\n",
      "Iteration 91, loss = 6.68566455\n",
      "Iteration 92, loss = 6.68703326\n",
      "Iteration 93, loss = 6.68261457\n",
      "Iteration 94, loss = 6.68770081\n",
      "Iteration 95, loss = 6.68312411\n",
      "Iteration 96, loss = 6.68067666\n",
      "Iteration 97, loss = 6.68196545\n",
      "Iteration 98, loss = 6.68004435\n",
      "Iteration 99, loss = 6.67544932\n",
      "Iteration 100, loss = 6.68073901\n",
      "Iteration 101, loss = 6.67893823\n",
      "Iteration 102, loss = 6.67543285\n",
      "Iteration 103, loss = 6.67551904\n",
      "Iteration 104, loss = 6.67288088\n",
      "Iteration 105, loss = 6.67044489\n",
      "Iteration 106, loss = 6.66814696\n",
      "Iteration 107, loss = 6.66701570\n",
      "Iteration 108, loss = 6.66517513\n",
      "Iteration 109, loss = 6.67207020\n",
      "Iteration 110, loss = 6.66778684\n",
      "Iteration 111, loss = 6.66988386\n",
      "Iteration 112, loss = 6.66133190\n",
      "Iteration 113, loss = 6.66287206\n",
      "Iteration 114, loss = 6.66732383\n",
      "Iteration 115, loss = 6.66568538\n",
      "Iteration 116, loss = 6.66283427\n",
      "Iteration 117, loss = 6.66371877\n",
      "Iteration 118, loss = 6.65616249\n",
      "Iteration 119, loss = 6.66004830\n",
      "Iteration 120, loss = 6.65481045\n",
      "Iteration 121, loss = 6.65491375\n",
      "Iteration 122, loss = 6.65832256\n",
      "Iteration 123, loss = 6.66184079\n",
      "Iteration 124, loss = 6.65141472\n",
      "Iteration 125, loss = 6.65034640\n",
      "Iteration 126, loss = 6.65180874\n",
      "Iteration 127, loss = 6.64990411\n",
      "Iteration 128, loss = 6.64718091\n",
      "Iteration 129, loss = 6.65230217\n",
      "Iteration 130, loss = 6.65052099\n",
      "Iteration 131, loss = 6.64687104\n",
      "Iteration 132, loss = 6.64666596\n",
      "Iteration 133, loss = 6.63924668\n",
      "Iteration 134, loss = 6.64744776\n",
      "Iteration 135, loss = 6.64248678\n",
      "Iteration 136, loss = 6.64209793\n",
      "Iteration 137, loss = 6.64585608\n",
      "Iteration 138, loss = 6.64313001\n",
      "Iteration 139, loss = 6.64248977\n",
      "Iteration 140, loss = 6.64064636\n",
      "Iteration 141, loss = 6.63561318\n",
      "Iteration 142, loss = 6.63552214\n",
      "Iteration 143, loss = 6.63641849\n",
      "Iteration 144, loss = 6.63494304\n",
      "Iteration 145, loss = 6.63276849\n",
      "Iteration 146, loss = 6.63442005\n",
      "Iteration 147, loss = 6.63376349\n",
      "Iteration 148, loss = 6.63223689\n",
      "Iteration 149, loss = 6.63273983\n",
      "Iteration 150, loss = 6.63252196\n",
      "Iteration 151, loss = 6.62847829\n",
      "Iteration 152, loss = 6.63283597\n",
      "Iteration 153, loss = 6.62851688\n",
      "Iteration 154, loss = 6.62932732\n",
      "Iteration 155, loss = 6.63304327\n",
      "Iteration 156, loss = 6.62453158\n",
      "Iteration 157, loss = 6.62614560\n",
      "Iteration 158, loss = 6.62545482\n",
      "Iteration 159, loss = 6.62520589\n",
      "Iteration 160, loss = 6.62685148\n",
      "Iteration 161, loss = 6.62384189\n",
      "Iteration 162, loss = 6.62169682\n",
      "Iteration 163, loss = 6.62258510\n",
      "Iteration 164, loss = 6.62098132\n",
      "Iteration 165, loss = 6.61659665\n",
      "Iteration 166, loss = 6.61985076\n",
      "Iteration 167, loss = 6.62392261\n",
      "Iteration 168, loss = 6.61951344\n",
      "Iteration 169, loss = 6.62151068\n",
      "Iteration 170, loss = 6.62083338\n",
      "Iteration 171, loss = 6.61756901\n",
      "Iteration 172, loss = 6.62149690\n",
      "Iteration 173, loss = 6.61788723\n",
      "Iteration 174, loss = 6.62198574\n",
      "Iteration 175, loss = 6.61503230\n",
      "Iteration 176, loss = 6.61693730\n",
      "Iteration 177, loss = 6.61409031\n",
      "Iteration 178, loss = 6.61505805\n",
      "Iteration 179, loss = 6.61512390\n",
      "Iteration 180, loss = 6.61611727\n",
      "Iteration 181, loss = 6.61225801\n",
      "Iteration 182, loss = 6.60892852\n",
      "Iteration 183, loss = 6.61232726\n",
      "Iteration 184, loss = 6.61342078\n",
      "Iteration 185, loss = 6.61030612\n",
      "Iteration 186, loss = 6.60624914\n",
      "Iteration 187, loss = 6.61081399\n",
      "Iteration 188, loss = 6.60613016\n",
      "Iteration 189, loss = 6.60593145\n",
      "Iteration 190, loss = 6.60772440\n",
      "Iteration 191, loss = 6.60762570\n",
      "Iteration 192, loss = 6.60550774\n",
      "Iteration 193, loss = 6.60508586\n",
      "Iteration 194, loss = 6.60503752\n",
      "Iteration 195, loss = 6.60428027\n",
      "Iteration 196, loss = 6.60539967\n",
      "Iteration 197, loss = 6.60181586\n",
      "Iteration 198, loss = 6.60635289\n",
      "Iteration 199, loss = 6.60103523\n",
      "Iteration 200, loss = 6.60340907\n",
      "Iteration 201, loss = 6.59959759\n",
      "Iteration 202, loss = 6.60381650\n",
      "Iteration 203, loss = 6.60432684\n",
      "Iteration 204, loss = 6.60222032\n",
      "Iteration 205, loss = 6.59605933\n",
      "Iteration 206, loss = 6.60072329\n",
      "Iteration 207, loss = 6.59824746\n",
      "Iteration 208, loss = 6.59835888\n",
      "Iteration 209, loss = 6.59902483\n",
      "Iteration 210, loss = 6.60109315\n",
      "Iteration 211, loss = 6.59981775\n",
      "Iteration 212, loss = 6.59828096\n",
      "Iteration 213, loss = 6.59662975\n",
      "Iteration 214, loss = 6.59895812\n",
      "Iteration 215, loss = 6.59355465\n",
      "Iteration 216, loss = 6.59619883\n",
      "Iteration 217, loss = 6.59648352\n",
      "Iteration 218, loss = 6.59409540\n",
      "Iteration 219, loss = 6.59223391\n",
      "Iteration 220, loss = 6.59159508\n",
      "Iteration 221, loss = 6.58818261\n",
      "Iteration 222, loss = 6.59563353\n",
      "Iteration 223, loss = 6.59212180\n",
      "Iteration 224, loss = 6.58762206\n",
      "Iteration 225, loss = 6.58899452\n",
      "Iteration 226, loss = 6.58888660\n",
      "Iteration 227, loss = 6.59362612\n",
      "Iteration 228, loss = 6.58843893\n",
      "Iteration 229, loss = 6.58336818\n",
      "Iteration 230, loss = 6.58876237\n",
      "Iteration 231, loss = 6.58755888\n",
      "Iteration 232, loss = 6.58935670\n",
      "Iteration 233, loss = 6.58388208\n",
      "Iteration 234, loss = 6.58496479\n",
      "Iteration 235, loss = 6.58427080\n",
      "Iteration 236, loss = 6.58419434\n",
      "Iteration 237, loss = 6.58724602\n",
      "Iteration 238, loss = 6.58243111\n",
      "Iteration 239, loss = 6.58391294\n",
      "Iteration 240, loss = 6.58462656\n",
      "Iteration 241, loss = 6.58014931\n",
      "Iteration 242, loss = 6.58352505\n",
      "Iteration 243, loss = 6.58419474\n",
      "Iteration 244, loss = 6.57680943\n",
      "Iteration 245, loss = 6.58333990\n",
      "Iteration 246, loss = 6.57813222\n",
      "Iteration 247, loss = 6.57264512\n",
      "Iteration 248, loss = 6.57869121\n",
      "Iteration 249, loss = 6.57533621\n",
      "Iteration 250, loss = 6.57834937\n",
      "[CV 2/3] END activation=relu, hidden_layer_sizes=(50,), learning_rate_init=0.001, max_iter=250;, score=-12.673 total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Downloads\\IMF\\Modulos\\Aun por resolver\\TFM\\TFM\\calories\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 999.51995219\n",
      "Iteration 2, loss = 23.81257880\n",
      "Iteration 3, loss = 10.35640592\n",
      "Iteration 4, loss = 7.89316064\n",
      "Iteration 5, loss = 7.37311639\n",
      "Iteration 6, loss = 7.12545310\n",
      "Iteration 7, loss = 6.99245033\n",
      "Iteration 8, loss = 6.91645898\n",
      "Iteration 9, loss = 6.86653537\n",
      "Iteration 10, loss = 6.83187713\n",
      "Iteration 11, loss = 6.80809632\n",
      "Iteration 12, loss = 6.79240902\n",
      "Iteration 13, loss = 6.77926644\n",
      "Iteration 14, loss = 6.76145703\n",
      "Iteration 15, loss = 6.74756840\n",
      "Iteration 16, loss = 6.73879679\n",
      "Iteration 17, loss = 6.72991034\n",
      "Iteration 18, loss = 6.71948181\n",
      "Iteration 19, loss = 6.71496991\n",
      "Iteration 20, loss = 6.70998912\n",
      "Iteration 21, loss = 6.70472179\n",
      "Iteration 22, loss = 6.69919632\n",
      "Iteration 23, loss = 6.68984287\n",
      "Iteration 24, loss = 6.68712000\n",
      "Iteration 25, loss = 6.68103346\n",
      "Iteration 26, loss = 6.67762201\n",
      "Iteration 27, loss = 6.67580969\n",
      "Iteration 28, loss = 6.67083323\n",
      "Iteration 29, loss = 6.66721790\n",
      "Iteration 30, loss = 6.66512499\n",
      "Iteration 31, loss = 6.65625256\n",
      "Iteration 32, loss = 6.65664828\n",
      "Iteration 33, loss = 6.65557447\n",
      "Iteration 34, loss = 6.65047681\n",
      "Iteration 35, loss = 6.64731244\n",
      "Iteration 36, loss = 6.64134751\n",
      "Iteration 37, loss = 6.64334533\n",
      "Iteration 38, loss = 6.64181386\n",
      "Iteration 39, loss = 6.63738223\n",
      "Iteration 40, loss = 6.63788385\n",
      "Iteration 41, loss = 6.63005575\n",
      "Iteration 42, loss = 6.63270056\n",
      "Iteration 43, loss = 6.62865980\n",
      "Iteration 44, loss = 6.62839422\n",
      "Iteration 45, loss = 6.62992350\n",
      "Iteration 46, loss = 6.63303297\n",
      "Iteration 47, loss = 6.62453150\n",
      "Iteration 48, loss = 6.62997557\n",
      "Iteration 49, loss = 6.62063355\n",
      "Iteration 50, loss = 6.62185996\n",
      "Iteration 51, loss = 6.62020733\n",
      "Iteration 52, loss = 6.62002728\n",
      "Iteration 53, loss = 6.61443353\n",
      "Iteration 54, loss = 6.61686379\n",
      "Iteration 55, loss = 6.62040255\n",
      "Iteration 56, loss = 6.61791578\n",
      "Iteration 57, loss = 6.61267522\n",
      "Iteration 58, loss = 6.61639241\n",
      "Iteration 59, loss = 6.61134083\n",
      "Iteration 60, loss = 6.61004947\n",
      "Iteration 61, loss = 6.61011271\n",
      "Iteration 62, loss = 6.60874446\n",
      "Iteration 63, loss = 6.61051640\n",
      "Iteration 64, loss = 6.60574292\n",
      "Iteration 65, loss = 6.60953891\n",
      "Iteration 66, loss = 6.61222440\n",
      "Iteration 67, loss = 6.60590126\n",
      "Iteration 68, loss = 6.60295296\n",
      "Iteration 69, loss = 6.60886770\n",
      "Iteration 70, loss = 6.60630222\n",
      "Iteration 71, loss = 6.60635195\n",
      "Iteration 72, loss = 6.59597759\n",
      "Iteration 73, loss = 6.60218399\n",
      "Iteration 74, loss = 6.60177384\n",
      "Iteration 75, loss = 6.59819814\n",
      "Iteration 76, loss = 6.59836777\n",
      "Iteration 77, loss = 6.59405425\n",
      "Iteration 78, loss = 6.59248026\n",
      "Iteration 79, loss = 6.59785993\n",
      "Iteration 80, loss = 6.58899401\n",
      "Iteration 81, loss = 6.59063887\n",
      "Iteration 82, loss = 6.59051166\n",
      "Iteration 83, loss = 6.58748387\n",
      "Iteration 84, loss = 6.58906382\n",
      "Iteration 85, loss = 6.58830837\n",
      "Iteration 86, loss = 6.58805993\n",
      "Iteration 87, loss = 6.58536301\n",
      "Iteration 88, loss = 6.58424517\n",
      "Iteration 89, loss = 6.58478768\n",
      "Iteration 90, loss = 6.58147523\n",
      "Iteration 91, loss = 6.58006670\n",
      "Iteration 92, loss = 6.57524544\n",
      "Iteration 93, loss = 6.58196376\n",
      "Iteration 94, loss = 6.58092473\n",
      "Iteration 95, loss = 6.57515828\n",
      "Iteration 96, loss = 6.57381736\n",
      "Iteration 97, loss = 6.57282794\n",
      "Iteration 98, loss = 6.57266576\n",
      "Iteration 99, loss = 6.57234151\n",
      "Iteration 100, loss = 6.57034405\n",
      "Iteration 101, loss = 6.56557624\n",
      "Iteration 102, loss = 6.56984992\n",
      "Iteration 103, loss = 6.56782421\n",
      "Iteration 104, loss = 6.56402504\n",
      "Iteration 105, loss = 6.56477674\n",
      "Iteration 106, loss = 6.56311031\n",
      "Iteration 107, loss = 6.55711363\n",
      "Iteration 108, loss = 6.55535719\n",
      "Iteration 109, loss = 6.56251220\n",
      "Iteration 110, loss = 6.55669082\n",
      "Iteration 111, loss = 6.55884278\n",
      "Iteration 112, loss = 6.55601969\n",
      "Iteration 113, loss = 6.55737842\n",
      "Iteration 114, loss = 6.55797829\n",
      "Iteration 115, loss = 6.55265405\n",
      "Iteration 116, loss = 6.55316260\n",
      "Iteration 117, loss = 6.55398315\n",
      "Iteration 118, loss = 6.55008237\n",
      "Iteration 119, loss = 6.55168272\n",
      "Iteration 120, loss = 6.54688562\n",
      "Iteration 121, loss = 6.54851811\n",
      "Iteration 122, loss = 6.55000565\n",
      "Iteration 123, loss = 6.55011447\n",
      "Iteration 124, loss = 6.54775508\n",
      "Iteration 125, loss = 6.54323599\n",
      "Iteration 126, loss = 6.54417147\n",
      "Iteration 127, loss = 6.54413949\n",
      "Iteration 128, loss = 6.54440850\n",
      "Iteration 129, loss = 6.53929772\n",
      "Iteration 130, loss = 6.54166955\n",
      "Iteration 131, loss = 6.54277676\n",
      "Iteration 132, loss = 6.54648325\n",
      "Iteration 133, loss = 6.53651332\n",
      "Iteration 134, loss = 6.54151166\n",
      "Iteration 135, loss = 6.54030726\n",
      "Iteration 136, loss = 6.53544194\n",
      "Iteration 137, loss = 6.53648516\n",
      "Iteration 138, loss = 6.53963851\n",
      "Iteration 139, loss = 6.53620739\n",
      "Iteration 140, loss = 6.53555528\n",
      "Iteration 141, loss = 6.53592275\n",
      "Iteration 142, loss = 6.53339571\n",
      "Iteration 143, loss = 6.53253392\n",
      "Iteration 144, loss = 6.53120327\n",
      "Iteration 145, loss = 6.53229485\n",
      "Iteration 146, loss = 6.53098320\n",
      "Iteration 147, loss = 6.52787043\n",
      "Iteration 148, loss = 6.52498374\n",
      "Iteration 149, loss = 6.53261987\n",
      "Iteration 150, loss = 6.52956975\n",
      "Iteration 151, loss = 6.52538659\n",
      "Iteration 152, loss = 6.52788034\n",
      "Iteration 153, loss = 6.52428805\n",
      "Iteration 154, loss = 6.52645635\n",
      "Iteration 155, loss = 6.52302843\n",
      "Iteration 156, loss = 6.52278675\n",
      "Iteration 157, loss = 6.52119801\n",
      "Iteration 158, loss = 6.52337011\n",
      "Iteration 159, loss = 6.52766196\n",
      "Iteration 160, loss = 6.52416981\n",
      "Iteration 161, loss = 6.52637263\n",
      "Iteration 162, loss = 6.52194643\n",
      "Iteration 163, loss = 6.52158112\n",
      "Iteration 164, loss = 6.52146577\n",
      "Iteration 165, loss = 6.51985776\n",
      "Iteration 166, loss = 6.52086545\n",
      "Iteration 167, loss = 6.51588012\n",
      "Iteration 168, loss = 6.52051508\n",
      "Iteration 169, loss = 6.52168086\n",
      "Iteration 170, loss = 6.51561361\n",
      "Iteration 171, loss = 6.51865144\n",
      "Iteration 172, loss = 6.52283049\n",
      "Iteration 173, loss = 6.51930660\n",
      "Iteration 174, loss = 6.51582993\n",
      "Iteration 175, loss = 6.51539519\n",
      "Iteration 176, loss = 6.51660057\n",
      "Iteration 177, loss = 6.51568318\n",
      "Iteration 178, loss = 6.51554271\n",
      "Iteration 179, loss = 6.52188558\n",
      "Iteration 180, loss = 6.51559144\n",
      "Iteration 181, loss = 6.51491444\n",
      "Iteration 182, loss = 6.51348712\n",
      "Iteration 183, loss = 6.51889791\n",
      "Iteration 184, loss = 6.51548919\n",
      "Iteration 185, loss = 6.51099415\n",
      "Iteration 186, loss = 6.51482697\n",
      "Iteration 187, loss = 6.51364347\n",
      "Iteration 188, loss = 6.51138755\n",
      "Iteration 189, loss = 6.51250837\n",
      "Iteration 190, loss = 6.51367398\n",
      "Iteration 191, loss = 6.51407275\n",
      "Iteration 192, loss = 6.51130092\n",
      "Iteration 193, loss = 6.51284778\n",
      "Iteration 194, loss = 6.51041935\n",
      "Iteration 195, loss = 6.50907151\n",
      "Iteration 196, loss = 6.51184490\n",
      "Iteration 197, loss = 6.51079621\n",
      "Iteration 198, loss = 6.50991316\n",
      "Iteration 199, loss = 6.51344718\n",
      "Iteration 200, loss = 6.50787594\n",
      "Iteration 201, loss = 6.50947190\n",
      "Iteration 202, loss = 6.50718572\n",
      "Iteration 203, loss = 6.50657871\n",
      "Iteration 204, loss = 6.50889245\n",
      "Iteration 205, loss = 6.50718494\n",
      "Iteration 206, loss = 6.50956997\n",
      "Iteration 207, loss = 6.50696029\n",
      "Iteration 208, loss = 6.50772757\n",
      "Iteration 209, loss = 6.50472530\n",
      "Iteration 210, loss = 6.50871361\n",
      "Iteration 211, loss = 6.50527594\n",
      "Iteration 212, loss = 6.50471189\n",
      "Iteration 213, loss = 6.50297393\n",
      "Iteration 214, loss = 6.50793754\n",
      "Iteration 215, loss = 6.50429233\n",
      "Iteration 216, loss = 6.50569882\n",
      "Iteration 217, loss = 6.50471539\n",
      "Iteration 218, loss = 6.50195161\n",
      "Iteration 219, loss = 6.50317181\n",
      "Iteration 220, loss = 6.50716109\n",
      "Iteration 221, loss = 6.50207333\n",
      "Iteration 222, loss = 6.50565411\n",
      "Iteration 223, loss = 6.50790627\n",
      "Iteration 224, loss = 6.50847305\n",
      "Iteration 225, loss = 6.49991658\n",
      "Iteration 226, loss = 6.50469703\n",
      "Iteration 227, loss = 6.50210045\n",
      "Iteration 228, loss = 6.50391600\n",
      "Iteration 229, loss = 6.49756448\n",
      "Iteration 230, loss = 6.50346380\n",
      "Iteration 231, loss = 6.50592071\n",
      "Iteration 232, loss = 6.50694202\n",
      "Iteration 233, loss = 6.50045235\n",
      "Iteration 234, loss = 6.50032227\n",
      "Iteration 235, loss = 6.49848305\n",
      "Iteration 236, loss = 6.50188531\n",
      "Iteration 237, loss = 6.50197236\n",
      "Iteration 238, loss = 6.49792370\n",
      "Iteration 239, loss = 6.50677463\n",
      "Iteration 240, loss = 6.50578694\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3] END activation=relu, hidden_layer_sizes=(50,), learning_rate_init=0.001, max_iter=250;, score=-12.972 total time= 3.6min\n",
      "Iteration 1, loss = 192.50598744\n",
      "Iteration 2, loss = 7.25428230\n",
      "Iteration 3, loss = 6.88754229\n",
      "Iteration 4, loss = 6.82742791\n",
      "Iteration 5, loss = 6.79928505\n",
      "Iteration 6, loss = 6.76399229\n",
      "Iteration 7, loss = 6.73697787\n",
      "Iteration 8, loss = 6.71310038\n",
      "Iteration 9, loss = 6.71650279\n",
      "Iteration 10, loss = 6.70465416\n",
      "Iteration 11, loss = 6.69612667\n",
      "Iteration 12, loss = 6.67537325\n",
      "Iteration 13, loss = 6.66687715\n",
      "Iteration 14, loss = 6.67156175\n",
      "Iteration 15, loss = 6.65343392\n",
      "Iteration 16, loss = 6.67602862\n",
      "Iteration 17, loss = 6.64305461\n",
      "Iteration 18, loss = 6.65220615\n",
      "Iteration 19, loss = 6.63749977\n",
      "Iteration 20, loss = 6.63450672\n",
      "Iteration 21, loss = 6.63159535\n",
      "Iteration 22, loss = 6.62249441\n",
      "Iteration 23, loss = 6.62641062\n",
      "Iteration 24, loss = 6.63219451\n",
      "Iteration 25, loss = 6.62204306\n",
      "Iteration 26, loss = 6.62111633\n",
      "Iteration 27, loss = 6.62297957\n",
      "Iteration 28, loss = 6.60435043\n",
      "Iteration 29, loss = 6.60971915\n",
      "Iteration 30, loss = 6.60919397\n",
      "Iteration 31, loss = 6.60084073\n",
      "Iteration 32, loss = 6.59310872\n",
      "Iteration 33, loss = 6.57989069\n",
      "Iteration 34, loss = 6.60630962\n",
      "Iteration 35, loss = 6.57087432\n",
      "Iteration 36, loss = 6.60469079\n",
      "Iteration 37, loss = 6.58366690\n",
      "Iteration 38, loss = 6.59719545\n",
      "Iteration 39, loss = 6.58112977\n",
      "Iteration 40, loss = 6.58371856\n",
      "Iteration 41, loss = 6.57453969\n",
      "Iteration 42, loss = 6.56505377\n",
      "Iteration 43, loss = 6.54879042\n",
      "Iteration 44, loss = 6.57388190\n",
      "Iteration 45, loss = 6.56217916\n",
      "Iteration 46, loss = 6.57466728\n",
      "Iteration 47, loss = 6.56069534\n",
      "Iteration 48, loss = 6.55287753\n",
      "Iteration 49, loss = 6.56006964\n",
      "Iteration 50, loss = 6.56863257\n",
      "Iteration 51, loss = 6.54868432\n",
      "Iteration 52, loss = 6.54748705\n",
      "Iteration 53, loss = 6.54319883\n",
      "Iteration 54, loss = 6.53085568\n",
      "Iteration 55, loss = 6.55063503\n",
      "Iteration 56, loss = 6.54097142\n",
      "Iteration 57, loss = 6.54591069\n",
      "Iteration 58, loss = 6.52177941\n",
      "Iteration 59, loss = 6.55792531\n",
      "Iteration 60, loss = 6.53276990\n",
      "Iteration 61, loss = 6.53727699\n",
      "Iteration 62, loss = 6.53125965\n",
      "Iteration 63, loss = 6.52070511\n",
      "Iteration 64, loss = 6.54506490\n",
      "Iteration 65, loss = 6.53588103\n",
      "Iteration 66, loss = 6.51124146\n",
      "Iteration 67, loss = 6.51117259\n",
      "Iteration 68, loss = 6.51954711\n",
      "Iteration 69, loss = 6.51161856\n",
      "Iteration 70, loss = 6.52172375\n",
      "Iteration 71, loss = 6.52301093\n",
      "Iteration 72, loss = 6.51789676\n",
      "Iteration 73, loss = 6.49841159\n",
      "Iteration 74, loss = 6.48408938\n",
      "Iteration 75, loss = 6.53333998\n",
      "Iteration 76, loss = 6.51251129\n",
      "Iteration 77, loss = 6.50120708\n",
      "Iteration 78, loss = 6.49471588\n",
      "Iteration 79, loss = 6.50843499\n",
      "Iteration 80, loss = 6.47455425\n",
      "Iteration 81, loss = 6.49802414\n",
      "Iteration 82, loss = 6.49655568\n",
      "Iteration 83, loss = 6.50228755\n",
      "Iteration 84, loss = 6.49031455\n",
      "Iteration 85, loss = 6.50749338\n",
      "Iteration 86, loss = 6.48184683\n",
      "Iteration 87, loss = 6.49300665\n",
      "Iteration 88, loss = 6.51354188\n",
      "Iteration 89, loss = 6.50348838\n",
      "Iteration 90, loss = 6.48998855\n",
      "Iteration 91, loss = 6.49618714\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/3] END activation=tanh, hidden_layer_sizes=(100,), learning_rate_init=0.01, max_iter=250;, score=-13.917 total time= 2.4min\n",
      "Iteration 1, loss = 193.03228150\n",
      "Iteration 2, loss = 7.49481529\n",
      "Iteration 3, loss = 7.13339079\n",
      "Iteration 4, loss = 7.06518538\n",
      "Iteration 5, loss = 7.04568770\n",
      "Iteration 6, loss = 7.00727737\n",
      "Iteration 7, loss = 6.97041895\n",
      "Iteration 8, loss = 6.96488876\n",
      "Iteration 9, loss = 6.94116776\n",
      "Iteration 10, loss = 6.90278938\n",
      "Iteration 11, loss = 6.90467129\n",
      "Iteration 12, loss = 6.89799647\n",
      "Iteration 13, loss = 6.88810195\n",
      "Iteration 14, loss = 6.86641153\n",
      "Iteration 15, loss = 6.85858291\n",
      "Iteration 16, loss = 6.86555434\n",
      "Iteration 17, loss = 6.83370759\n",
      "Iteration 18, loss = 6.86740685\n",
      "Iteration 19, loss = 6.85619537\n",
      "Iteration 20, loss = 6.82548466\n",
      "Iteration 21, loss = 6.84794177\n",
      "Iteration 22, loss = 6.83388367\n",
      "Iteration 23, loss = 6.82750681\n",
      "Iteration 24, loss = 6.81010258\n",
      "Iteration 25, loss = 6.81929854\n",
      "Iteration 26, loss = 6.80946975\n",
      "Iteration 27, loss = 6.80789589\n",
      "Iteration 28, loss = 6.79439750\n",
      "Iteration 29, loss = 6.78018939\n",
      "Iteration 30, loss = 6.79523316\n",
      "Iteration 31, loss = 6.78239225\n",
      "Iteration 32, loss = 6.77549831\n",
      "Iteration 33, loss = 6.79131688\n",
      "Iteration 34, loss = 6.76682309\n",
      "Iteration 35, loss = 6.78112803\n",
      "Iteration 36, loss = 6.77409414\n",
      "Iteration 37, loss = 6.77199584\n",
      "Iteration 38, loss = 6.77570468\n",
      "Iteration 39, loss = 6.77186067\n",
      "Iteration 40, loss = 6.75507539\n",
      "Iteration 41, loss = 6.76490887\n",
      "Iteration 42, loss = 6.75393816\n",
      "Iteration 43, loss = 6.75604210\n",
      "Iteration 44, loss = 6.74136125\n",
      "Iteration 45, loss = 6.73696583\n",
      "Iteration 46, loss = 6.74931837\n",
      "Iteration 47, loss = 6.74894817\n",
      "Iteration 48, loss = 6.73893373\n",
      "Iteration 49, loss = 6.73692904\n",
      "Iteration 50, loss = 6.74414482\n",
      "Iteration 51, loss = 6.72023474\n",
      "Iteration 52, loss = 6.73095327\n",
      "Iteration 53, loss = 6.73234396\n",
      "Iteration 54, loss = 6.72662211\n",
      "Iteration 55, loss = 6.72295540\n",
      "Iteration 56, loss = 6.74555457\n",
      "Iteration 57, loss = 6.72554874\n",
      "Iteration 58, loss = 6.71866038\n",
      "Iteration 59, loss = 6.72630613\n",
      "Iteration 60, loss = 6.72184826\n",
      "Iteration 61, loss = 6.69776776\n",
      "Iteration 62, loss = 6.70140071\n",
      "Iteration 63, loss = 6.70576854\n",
      "Iteration 64, loss = 6.70752287\n",
      "Iteration 65, loss = 6.71784468\n",
      "Iteration 66, loss = 6.70574375\n",
      "Iteration 67, loss = 6.68057229\n",
      "Iteration 68, loss = 6.69243424\n",
      "Iteration 69, loss = 6.69822362\n",
      "Iteration 70, loss = 6.69437076\n",
      "Iteration 71, loss = 6.69847918\n",
      "Iteration 72, loss = 6.68475066\n",
      "Iteration 73, loss = 6.68395201\n",
      "Iteration 74, loss = 6.68651006\n",
      "Iteration 75, loss = 6.68906902\n",
      "Iteration 76, loss = 6.68181112\n",
      "Iteration 77, loss = 6.68531747\n",
      "Iteration 78, loss = 6.68601256\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/3] END activation=tanh, hidden_layer_sizes=(100,), learning_rate_init=0.01, max_iter=250;, score=-13.049 total time= 2.2min\n",
      "Iteration 1, loss = 194.56992707\n",
      "Iteration 2, loss = 7.41295163\n",
      "Iteration 3, loss = 7.04034259\n",
      "Iteration 4, loss = 6.99113967\n",
      "Iteration 5, loss = 6.93880771\n",
      "Iteration 6, loss = 6.94286813\n",
      "Iteration 7, loss = 6.89449581\n",
      "Iteration 8, loss = 6.90984585\n",
      "Iteration 9, loss = 6.89480385\n",
      "Iteration 10, loss = 6.86984448\n",
      "Iteration 11, loss = 6.85259182\n",
      "Iteration 12, loss = 6.83358914\n",
      "Iteration 13, loss = 6.85585828\n",
      "Iteration 14, loss = 6.84293538\n",
      "Iteration 15, loss = 6.80578737\n",
      "Iteration 16, loss = 6.80510905\n",
      "Iteration 17, loss = 6.81032994\n",
      "Iteration 18, loss = 6.78863366\n",
      "Iteration 19, loss = 6.81325403\n",
      "Iteration 20, loss = 6.77220062\n",
      "Iteration 21, loss = 6.79460660\n",
      "Iteration 22, loss = 6.74926782\n",
      "Iteration 23, loss = 6.75789496\n",
      "Iteration 24, loss = 6.73994105\n",
      "Iteration 25, loss = 6.73438124\n",
      "Iteration 26, loss = 6.74130184\n",
      "Iteration 27, loss = 6.74599387\n",
      "Iteration 28, loss = 6.72640068\n",
      "Iteration 29, loss = 6.73096851\n",
      "Iteration 30, loss = 6.73078291\n",
      "Iteration 31, loss = 6.72163979\n",
      "Iteration 32, loss = 6.71912352\n",
      "Iteration 33, loss = 6.71453904\n",
      "Iteration 34, loss = 6.70475993\n",
      "Iteration 35, loss = 6.72343742\n",
      "Iteration 36, loss = 6.69015357\n",
      "Iteration 37, loss = 6.71100558\n",
      "Iteration 38, loss = 6.70657566\n",
      "Iteration 39, loss = 6.69544497\n",
      "Iteration 40, loss = 6.67482106\n",
      "Iteration 41, loss = 6.68660812\n",
      "Iteration 42, loss = 6.67448102\n",
      "Iteration 43, loss = 6.68554744\n",
      "Iteration 44, loss = 6.67248411\n",
      "Iteration 45, loss = 6.65150901\n",
      "Iteration 46, loss = 6.68958561\n",
      "Iteration 47, loss = 6.67898392\n",
      "Iteration 48, loss = 6.66160333\n",
      "Iteration 49, loss = 6.65097992\n",
      "Iteration 50, loss = 6.65571278\n",
      "Iteration 51, loss = 6.64043449\n",
      "Iteration 52, loss = 6.64546041\n",
      "Iteration 53, loss = 6.65579637\n",
      "Iteration 54, loss = 6.63150212\n",
      "Iteration 55, loss = 6.63501016\n",
      "Iteration 56, loss = 6.63735245\n",
      "Iteration 57, loss = 6.63725790\n",
      "Iteration 58, loss = 6.63821896\n",
      "Iteration 59, loss = 6.63238969\n",
      "Iteration 60, loss = 6.61542796\n",
      "Iteration 61, loss = 6.62542542\n",
      "Iteration 62, loss = 6.60574589\n",
      "Iteration 63, loss = 6.59753526\n",
      "Iteration 64, loss = 6.62484180\n",
      "Iteration 65, loss = 6.60411748\n",
      "Iteration 66, loss = 6.60701236\n",
      "Iteration 67, loss = 6.62311122\n",
      "Iteration 68, loss = 6.61421725\n",
      "Iteration 69, loss = 6.60751975\n",
      "Iteration 70, loss = 6.62839149\n",
      "Iteration 71, loss = 6.60245521\n",
      "Iteration 72, loss = 6.61165669\n",
      "Iteration 73, loss = 6.62821513\n",
      "Iteration 74, loss = 6.58973051\n",
      "Iteration 75, loss = 6.60447353\n",
      "Iteration 76, loss = 6.57995778\n",
      "Iteration 77, loss = 6.60366109\n",
      "Iteration 78, loss = 6.59555960\n",
      "Iteration 79, loss = 6.58218180\n",
      "Iteration 80, loss = 6.58489528\n",
      "Iteration 81, loss = 6.59181153\n",
      "Iteration 82, loss = 6.58126876\n",
      "Iteration 83, loss = 6.57889041\n",
      "Iteration 84, loss = 6.58698515\n",
      "Iteration 85, loss = 6.56800733\n",
      "Iteration 86, loss = 6.57833477\n",
      "Iteration 87, loss = 6.58336054\n",
      "Iteration 88, loss = 6.59084541\n",
      "Iteration 89, loss = 6.55028987\n",
      "Iteration 90, loss = 6.53778120\n",
      "Iteration 91, loss = 6.57699274\n",
      "Iteration 92, loss = 6.56189427\n",
      "Iteration 93, loss = 6.58482315\n",
      "Iteration 94, loss = 6.54030815\n",
      "Iteration 95, loss = 6.56922618\n",
      "Iteration 96, loss = 6.53363238\n",
      "Iteration 97, loss = 6.57945846\n",
      "Iteration 98, loss = 6.54945193\n",
      "Iteration 99, loss = 6.53647987\n",
      "Iteration 100, loss = 6.56193288\n",
      "Iteration 101, loss = 6.54851305\n",
      "Iteration 102, loss = 6.55004008\n",
      "Iteration 103, loss = 6.56974996\n",
      "Iteration 104, loss = 6.52930710\n",
      "Iteration 105, loss = 6.54939271\n",
      "Iteration 106, loss = 6.52674680\n",
      "Iteration 107, loss = 6.55722377\n",
      "Iteration 108, loss = 6.51866926\n",
      "Iteration 109, loss = 6.51859807\n",
      "Iteration 110, loss = 6.55091094\n",
      "Iteration 111, loss = 6.53478753\n",
      "Iteration 112, loss = 6.52557772\n",
      "Iteration 113, loss = 6.52443930\n",
      "Iteration 114, loss = 6.55080999\n",
      "Iteration 115, loss = 6.52999465\n",
      "Iteration 116, loss = 6.50172247\n",
      "Iteration 117, loss = 6.50638239\n",
      "Iteration 118, loss = 6.52172250\n",
      "Iteration 119, loss = 6.50556696\n",
      "Iteration 120, loss = 6.51644590\n",
      "Iteration 121, loss = 6.52718980\n",
      "Iteration 122, loss = 6.51597875\n",
      "Iteration 123, loss = 6.50434114\n",
      "Iteration 124, loss = 6.53001693\n",
      "Iteration 125, loss = 6.52594466\n",
      "Iteration 126, loss = 6.51393400\n",
      "Iteration 127, loss = 6.52213025\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3] END activation=tanh, hidden_layer_sizes=(100,), learning_rate_init=0.01, max_iter=250;, score=-13.300 total time= 3.6min\n",
      "Iteration 1, loss = 1074.54100727\n",
      "Iteration 2, loss = 35.65155210\n",
      "Iteration 3, loss = 9.45911294\n",
      "Iteration 4, loss = 7.02464304\n",
      "Iteration 5, loss = 6.73615516\n",
      "Iteration 6, loss = 6.66698712\n",
      "Iteration 7, loss = 6.63565504\n",
      "Iteration 8, loss = 6.61540633\n",
      "Iteration 9, loss = 6.59421331\n",
      "Iteration 10, loss = 6.57821154\n",
      "Iteration 11, loss = 6.56240063\n",
      "Iteration 12, loss = 6.54866449\n",
      "Iteration 13, loss = 6.53937441\n",
      "Iteration 14, loss = 6.52630283\n",
      "Iteration 15, loss = 6.51942730\n",
      "Iteration 16, loss = 6.50819354\n",
      "Iteration 17, loss = 6.50196597\n",
      "Iteration 18, loss = 6.48881038\n",
      "Iteration 19, loss = 6.48265790\n",
      "Iteration 20, loss = 6.46975857\n",
      "Iteration 21, loss = 6.46288779\n",
      "Iteration 22, loss = 6.45454476\n",
      "Iteration 23, loss = 6.44621591\n",
      "Iteration 24, loss = 6.44253915\n",
      "Iteration 25, loss = 6.43329479\n",
      "Iteration 26, loss = 6.42767752\n",
      "Iteration 27, loss = 6.42582892\n",
      "Iteration 28, loss = 6.41988104\n",
      "Iteration 29, loss = 6.41588858\n",
      "Iteration 30, loss = 6.40980967\n",
      "Iteration 31, loss = 6.40444723\n",
      "Iteration 32, loss = 6.40312634\n",
      "Iteration 33, loss = 6.39938611\n",
      "Iteration 34, loss = 6.39938409\n",
      "Iteration 35, loss = 6.39223427\n",
      "Iteration 36, loss = 6.38871494\n",
      "Iteration 37, loss = 6.38669817\n",
      "Iteration 38, loss = 6.38298584\n",
      "Iteration 39, loss = 6.37758235\n",
      "Iteration 40, loss = 6.37731756\n",
      "Iteration 41, loss = 6.37588470\n",
      "Iteration 42, loss = 6.37591690\n",
      "Iteration 43, loss = 6.37030597\n",
      "Iteration 44, loss = 6.36736019\n",
      "Iteration 45, loss = 6.36586228\n",
      "Iteration 46, loss = 6.36173155\n",
      "Iteration 47, loss = 6.36522220\n",
      "Iteration 48, loss = 6.36461271\n",
      "Iteration 49, loss = 6.35776740\n",
      "Iteration 50, loss = 6.36016066\n",
      "Iteration 51, loss = 6.35690580\n",
      "Iteration 52, loss = 6.35565010\n",
      "Iteration 53, loss = 6.34971122\n",
      "Iteration 54, loss = 6.35402587\n",
      "Iteration 55, loss = 6.34919542\n",
      "Iteration 56, loss = 6.34687326\n",
      "Iteration 57, loss = 6.34870173\n",
      "Iteration 58, loss = 6.34758505\n",
      "Iteration 59, loss = 6.34399987\n",
      "Iteration 60, loss = 6.34215871\n",
      "Iteration 61, loss = 6.34379456\n",
      "Iteration 62, loss = 6.34279578\n",
      "Iteration 63, loss = 6.33600730\n",
      "Iteration 64, loss = 6.33616055\n",
      "Iteration 65, loss = 6.33889234\n",
      "Iteration 66, loss = 6.33526737\n",
      "Iteration 67, loss = 6.33331871\n",
      "Iteration 68, loss = 6.32979758\n",
      "Iteration 69, loss = 6.33024682\n",
      "Iteration 70, loss = 6.33034711\n",
      "Iteration 71, loss = 6.32682004\n",
      "Iteration 72, loss = 6.32633642\n",
      "Iteration 73, loss = 6.32735752\n",
      "Iteration 74, loss = 6.32960183\n",
      "Iteration 75, loss = 6.32597325\n",
      "Iteration 76, loss = 6.32252418\n",
      "Iteration 77, loss = 6.32548438\n",
      "Iteration 78, loss = 6.32044913\n",
      "Iteration 79, loss = 6.32097029\n",
      "Iteration 80, loss = 6.31895692\n",
      "Iteration 81, loss = 6.31722405\n",
      "Iteration 82, loss = 6.32010759\n",
      "Iteration 83, loss = 6.31652143\n",
      "Iteration 84, loss = 6.31364476\n",
      "Iteration 85, loss = 6.31505279\n",
      "Iteration 86, loss = 6.31480432\n",
      "Iteration 87, loss = 6.31020038\n",
      "Iteration 88, loss = 6.30984954\n",
      "Iteration 89, loss = 6.30917236\n",
      "Iteration 90, loss = 6.31017659\n",
      "Iteration 91, loss = 6.30573371\n",
      "Iteration 92, loss = 6.30863570\n",
      "Iteration 93, loss = 6.30788984\n",
      "Iteration 94, loss = 6.30454548\n",
      "Iteration 95, loss = 6.30458428\n",
      "Iteration 96, loss = 6.30184936\n",
      "Iteration 97, loss = 6.30235797\n",
      "Iteration 98, loss = 6.30185397\n",
      "Iteration 99, loss = 6.29977830\n",
      "Iteration 100, loss = 6.30312375\n",
      "Iteration 101, loss = 6.29930157\n",
      "Iteration 102, loss = 6.29808890\n",
      "Iteration 103, loss = 6.29584638\n",
      "Iteration 104, loss = 6.29870440\n",
      "Iteration 105, loss = 6.29432695\n",
      "Iteration 106, loss = 6.29683564\n",
      "Iteration 107, loss = 6.29291787\n",
      "Iteration 108, loss = 6.29151298\n",
      "Iteration 109, loss = 6.29258054\n",
      "Iteration 110, loss = 6.29181883\n",
      "Iteration 111, loss = 6.29307821\n",
      "Iteration 112, loss = 6.29388414\n",
      "Iteration 113, loss = 6.29689782\n",
      "Iteration 114, loss = 6.28771659\n",
      "Iteration 115, loss = 6.29141002\n",
      "Iteration 116, loss = 6.28838628\n",
      "Iteration 117, loss = 6.28619715\n",
      "Iteration 118, loss = 6.28798031\n",
      "Iteration 119, loss = 6.28618875\n",
      "Iteration 120, loss = 6.28396234\n",
      "Iteration 121, loss = 6.28642010\n",
      "Iteration 122, loss = 6.29108366\n",
      "Iteration 123, loss = 6.28211399\n",
      "Iteration 124, loss = 6.28117943\n",
      "Iteration 125, loss = 6.28288334\n",
      "Iteration 126, loss = 6.27744662\n",
      "Iteration 127, loss = 6.28090854\n",
      "Iteration 128, loss = 6.28046076\n",
      "Iteration 129, loss = 6.27738369\n",
      "Iteration 130, loss = 6.28205617\n",
      "Iteration 131, loss = 6.27440237\n",
      "Iteration 132, loss = 6.27735212\n",
      "Iteration 133, loss = 6.27677583\n",
      "Iteration 134, loss = 6.27705025\n",
      "Iteration 135, loss = 6.27608662\n",
      "Iteration 136, loss = 6.27332093\n",
      "Iteration 137, loss = 6.27752889\n",
      "Iteration 138, loss = 6.27569975\n",
      "Iteration 139, loss = 6.27795680\n",
      "Iteration 140, loss = 6.27671054\n",
      "Iteration 141, loss = 6.27290433\n",
      "Iteration 142, loss = 6.27291474\n",
      "Iteration 143, loss = 6.27300200\n",
      "Iteration 144, loss = 6.27257138\n",
      "Iteration 145, loss = 6.26953507\n",
      "Iteration 146, loss = 6.27023776\n",
      "Iteration 147, loss = 6.27350725\n",
      "Iteration 148, loss = 6.27062548\n",
      "Iteration 149, loss = 6.26911504\n",
      "Iteration 150, loss = 6.27013071\n",
      "Iteration 151, loss = 6.26882323\n",
      "Iteration 152, loss = 6.26862760\n",
      "Iteration 153, loss = 6.26790405\n",
      "Iteration 154, loss = 6.26866447\n",
      "Iteration 155, loss = 6.26727403\n",
      "Iteration 156, loss = 6.26732923\n",
      "Iteration 157, loss = 6.26425002\n",
      "Iteration 158, loss = 6.26445010\n",
      "Iteration 159, loss = 6.26256743\n",
      "Iteration 160, loss = 6.26165312\n",
      "Iteration 161, loss = 6.26214125\n",
      "Iteration 162, loss = 6.26194430\n",
      "Iteration 163, loss = 6.26192631\n",
      "Iteration 164, loss = 6.25955132\n",
      "Iteration 165, loss = 6.26210314\n",
      "Iteration 166, loss = 6.26202255\n",
      "Iteration 167, loss = 6.26239283\n",
      "Iteration 168, loss = 6.26023217\n",
      "Iteration 169, loss = 6.25986825\n",
      "Iteration 170, loss = 6.25862368\n",
      "Iteration 171, loss = 6.25950118\n",
      "Iteration 172, loss = 6.25593747\n",
      "Iteration 173, loss = 6.26128762\n",
      "Iteration 174, loss = 6.25699874\n",
      "Iteration 175, loss = 6.25628861\n",
      "Iteration 176, loss = 6.25738847\n",
      "Iteration 177, loss = 6.25520896\n",
      "Iteration 178, loss = 6.25962477\n",
      "Iteration 179, loss = 6.25512422\n",
      "Iteration 180, loss = 6.25367715\n",
      "Iteration 181, loss = 6.25675542\n",
      "Iteration 182, loss = 6.25177770\n",
      "Iteration 183, loss = 6.25308809\n",
      "Iteration 184, loss = 6.25241794\n",
      "Iteration 185, loss = 6.25124529\n",
      "Iteration 186, loss = 6.24859949\n",
      "Iteration 187, loss = 6.25335503\n",
      "Iteration 188, loss = 6.24952970\n",
      "Iteration 189, loss = 6.25095154\n",
      "Iteration 190, loss = 6.25261058\n",
      "Iteration 191, loss = 6.24875287\n",
      "Iteration 192, loss = 6.24958380\n",
      "Iteration 193, loss = 6.24961118\n",
      "Iteration 194, loss = 6.24771074\n",
      "Iteration 195, loss = 6.24764816\n",
      "Iteration 196, loss = 6.24760055\n",
      "Iteration 197, loss = 6.24607959\n",
      "Iteration 198, loss = 6.24634087\n",
      "Iteration 199, loss = 6.24319193\n",
      "Iteration 200, loss = 6.24531196\n",
      "Iteration 201, loss = 6.23979117\n",
      "Iteration 202, loss = 6.24363031\n",
      "Iteration 203, loss = 6.24023504\n",
      "Iteration 204, loss = 6.24291064\n",
      "Iteration 205, loss = 6.23907773\n",
      "Iteration 206, loss = 6.24059969\n",
      "Iteration 207, loss = 6.24028688\n",
      "Iteration 208, loss = 6.23882150\n",
      "Iteration 209, loss = 6.23999024\n",
      "Iteration 210, loss = 6.23896438\n",
      "Iteration 211, loss = 6.23635280\n",
      "Iteration 212, loss = 6.23986845\n",
      "Iteration 213, loss = 6.23728804\n",
      "Iteration 214, loss = 6.23572508\n",
      "Iteration 215, loss = 6.23646687\n",
      "Iteration 216, loss = 6.23182255\n",
      "Iteration 217, loss = 6.23346701\n",
      "Iteration 218, loss = 6.22788418\n",
      "Iteration 219, loss = 6.23571508\n",
      "Iteration 220, loss = 6.23326507\n",
      "Iteration 221, loss = 6.23056387\n",
      "Iteration 222, loss = 6.23488354\n",
      "Iteration 223, loss = 6.23170911\n",
      "Iteration 224, loss = 6.23219491\n",
      "Iteration 225, loss = 6.23046152\n",
      "Iteration 226, loss = 6.23367661\n",
      "Iteration 227, loss = 6.23235435\n",
      "Iteration 228, loss = 6.23356783\n",
      "Iteration 229, loss = 6.23003803\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "results = {}\n",
    "param_dist_mlp = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (100,50)],\n",
    "    \"activation\": [\"relu\", \"tanh\"],\n",
    "    \"learning_rate_init\": [0.001, 0.01],\n",
    "    \"max_iter\": [250, 300]\n",
    "}\n",
    "mlp_grid = RandomizedSearchCV(MLPRegressor(random_state=42, verbose=True), param_distributions=param_dist_mlp,\n",
    "                              n_iter=4, cv=3, scoring=\"neg_mean_squared_error\", n_jobs=1, random_state=42,\n",
    "                              verbose=3)\n",
    "mlp_grid.fit(X_train, y_train)  \n",
    "res, best = report_results(\"MLPRegressor\", mlp_grid, X_val, y_val)\n",
    "results[\"MLPRegressor\"] = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658840f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio si no existe\n",
    "os.makedirs(\"../../results/models\", exist_ok=True)\n",
    "joblib.dump(best, \"../../results/models/MLPRegressor.joblib\")\n",
    "append_result_to_csv(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e7e566b274f4b68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:33:13.855512Z",
     "start_time": "2025-09-20T15:33:13.836942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     model                                        best_params  \\\n",
      "MLPRegressor  MLPRegressor  {'max_iter': 250, 'learning_rate_init': 0.001,...   \n",
      "\n",
      "                   MAE      RMSE        R2  \n",
      "MLPRegressor  2.130965  3.567028  0.996717  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n",
    "results_df.to_csv(\"../../results/models/baseline_results_mlp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c00d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
